





















papersize=,






plain
thm
remark
rem[thm]





















showcaptionsetup
 caption=falsesubfig



Remark
Theorem



Spatial Wireless Channel Prediction

under Location Uncertainty


L. Srikar Muppirisetty, Tommy Svensson, Senior Member, IEEE,
and Henk Wymeersch, Member, IEEE 
The authors are with the Department of Signals and Systems, Chalmers
University of Technology, Gothenburg, Sweden, e-mail: srikar.muppirisetty, henkw, tommy.svensson@chalmers.se.

This research was supported, in part, by the European Research Council,
under Grant No. 258418 (COOPNET), and the Swedish Research Council
VR under the project Grant No. 621-2009-4555 (Dynamic Multipoint Wireless
Transmission).



Spatial wireless channel prediction is important for future wireless
networks, and in particular for proactive resource allocation at different
layers of the protocol stack. Various sources of uncertainty must
be accounted for during modeling and to provide robust predictions.
We investigate two channel prediction frameworks, classical Gaussian
processes (cGP) and uncertain Gaussian processes (uGP), and analyze
the impact of location uncertainty during learning/training and prediction/testing,
for scenarios where measurements uncertainty are dominated by large-scale
fading. We observe that cGP generally fails both in terms of learning
the channel parameters and in predicting the channel in the presence
of location uncertainties.In contrast, uGP explicitly
considers the location uncertainty. Using simulated data, we show
that uGP is able to learn and predict the wireless channel.


Gaussian processes, uncertain inputs, location uncertainty, spatial
predictability of wireless channels.



Introduction

Location-based resource allocation schemes are
expected to become an essential element of emerging 5G networks, as
5G devices will have the capability to accurately self-localize and
predict relevant channel quality metrics (CQM)
based on crowd-sourced databases. The geo-tagged CQM (including, e.g.,
received signal strength, delay spread, and interference levels) from
users enables the construction of a dynamic database, which in turn
allows the prediction of CQM at arbitrary locations and future times.
Current standards are already moving in this direction through the
so-called minimization of drive test (MDT) feature in 3GPPP Release
10. In MDT, users collect radio
measurements and associated location information in order to assess
network performance. In terms of applications, prediction of spatial
wireless channels (e.g., through radio environment maps) and its utilization
in resource allocation can reduce overheads and delays due to the
ability to predict channel quality beyond traditional time scales
. Exploitation of location-aware CQM is relevant
for interference management in two-tier cellular networks,
coverage hole detection and prediction,
cooperative spectrum sensing in cognitive radios,
anticipatory networks for predictive resource allocation,
and proactive caching.

In order to predict location-dependent radio propagation channels,
we rely on mathematical models, in which the physical environment,
including the locations of transmitter and receiver, play an important
role. The received signal power in a wireless channel is mainly affected
by three major dynamics, which occur at different length scales: path-loss,
shadowing, and small-scale fading. Small-scale
fading decorrelates within tens of centimeters (depending on the carrier
frequency), making it infeasible to predict based on location information.
On the other hand, shadowing is correlated up to tens of meters, depending
on the propagation environment (e.g., 50-100 m for outdoor
and 1-2 m for indoor environments).
Finally, path-loss, which captures the deterministic decay of power
with distance, is a deterministic function of the distance to the
transmitter. In rich scattering environments, the measurements average
small-scale fading either in frequency or space provided sufficient
bandwidth or number of antennas.
Thus, provided that measurements are dominated by large-scale fading,
location-dependent models for path-loss and shadowing can be developed
based on the physical properties of the wireless channel. With the
help of spatial regression tools, these large-scale channel components
can be predicted at other locations and used for resource allocation
. However, since localization is subject to
various error sources (e.g., the global positioning system (GPS) gives
an accuracy of around 10 m in outdoor scenarios,
while ultra-wide band (UWB) systems can give sub-meter accuracy),
there is a fundamental need to account for location uncertainties
when developing spatial regression tools.

Spatial regression tools generally comprise a training/learning phase,
in which the underlying channel parameters are estimated based on
the available training database, and a testing/prediction phase, in
which predictions are made at test locations, given learned parameters
and the training database. Among such tools, Gaussian processes (GP)
is a powerful and commonly used regression framework, since it is
generally considered to be the most flexible and provides prediction
uncertainty information. Two important
limitations of GP are its computational complexity
and its sensitivity to uncertain inputs.
To alleviate the computational complexity, various sparse GP techniques
have been proposed in,
while online and distributed GP were treated in
and,
respectively. The impact of input uncertainty was studied in,
which showed that GP was adversely affected, both in training and
testing, by input uncertainties. The input uncertainty in our case
corresponds to location uncertainty.

No framework has yet been developed to mathematically characterize
and understand the spatial predictability of wireless channels with
location uncertainty. In this paper, we build on and adapt the framework
from to CQM prediction
in wireless networks. Our main contributions are as follows:

We show that not considering location uncertainty leads to poor learning
of the channel parameters and poor prediction of CQM values at other
locations, especially when location uncertainties are heterogeneous;
We relate and unify existing GP methods that account for uncertainty
during both learning and prediction, by operating directly on an input
set of distributions, rather than an input set of locations;
We describe and delimit proper choices for mean functions and covariance
functions in this unified framework, so as to incorporate location
uncertainty in both learning and prediction; and
We demonstrate the use of the proposed framework for simulated data
and apply it to a spatial resource allocation application.

The remainder of the paper is structured as follows. Section 
presents the channel model and details the problem description for
location-dependent channel prediction with location uncertainty. In
Section , we review channel learning and prediction
in the classical GP (cGP) setup with no localization errors. Section
 details learning and prediction procedures using the
proposed GP framework that accounts for uncertainty on training and
test locations, termed uncertain GP (uGP). Finally, numerical results
are given in Section  in addition to a
resource allocation example, followed by our conclusions in Section
.


Notation

Vectors and matrices are written in bold (e.g., a vector 
and a matrix ); 
denotes transpose of ; 
denotes determinant of ; 
denotes entry  of ;  denotes
identity matrix of appropriate size;  and 
are vectors of ones and zeros, respectively, of appropriate size;
 denotes -norm unless otherwise stated; 
denotes the expectation operator;  denotes covariance
operator (i.e., );
 denotes a Gaussian
distribution evaluated in  with mean vector 
and covariance matrix  and 
denotes that  is drawn from a Gaussian distribution with
mean vector  and covariance matrix . Important
symbols used in the paper are: 
is an exact, true location; , 
is a vector that describes (e.g., in the form of moments) the location
distribution . For example in the case of Gaussian
distributed localization error, ,
then a possible choice is ,
where  stacks all the elements of
 in a vector. Finally, 
is a location estimate extracted from  through a
function  (e.g., the mean or mode).


Related Work

First, we give an overview of the literature on GP with uncertain
inputs. One way to deal with the input noise is through linearizing
the output around the mean of the input.
In, the input noise was viewed as extra
output noise by linearization at each point and this is proportional
to the squared gradient of the GP posterior mean. However, the proposed
method works under the condition of constant-variance input noise.
In, a Delta method was used for linearization
under the assumption of Gaussian distributed inputs and proposed a
corrected covariance function that accounts for the input noise variance.
For Gaussian distributed test inputs and known training inputs, the
exact and approximate moments of the GP posterior was examined for
various forms of covariance functions.
Training on Gaussian distributed input points by calculating the expected
covariance matrix was studied in.
Two approximations were evaluated in,
first a joint maximization of joint posterior on uncertain inputs
and hyperparameters (leading to over-fitting), and second using a
stochastic expectation-maximization algorithm (at a high computational
cost).

We now review previous works on GP for channel prediction, which include
spatial correlation of shadowing in cellular
and ad-hoc networks, as well as tracking
of transmit powers of primary users in a cognitive network.
In, GP was shown to model spatially
correlated shadowing to predict shadowing and path-loss at any arbitrary
location. A multi-hop network scenario was considered,
and shadowing was modeled using a spatial loss field, integrated along
a line between transmitter and receiver. In,
a cognitive network setting was evaluated, in which the transmit powers
of the primary users were tracked with cooperation among the secondary
users. For this purpose a distributed radio channel tracking framework
using Kriged Kalman filter was developed with location information.
A study on the impact of underlying channel parameters on the spatial
channel prediction variance using GP was presented in.
A common assumption in
was the presence of perfect location information. This assumption
was partially removed in, which extends
to include the effect of localization errors on spatial channel prediction.
It was found that channel prediction performance was degraded when
location errors were present, in particular when either the shadowing
standard deviation or the shadowing correlation were large. However,
 did not tackle combined learning and prediction
under location uncertainty. The only work that explicitly accounts
for location uncertainty was, in which
the Laplace approximation was used to obtain a closed-form analytical
solution for the posterior predictive distribution. However,
did not consider learning of parameters in presence of location uncertainty.


System Model 


Channel Model

Consider a geographical region ,
where a source node is located at the origin and transmits a signal
with power  to a receiver located at 
through a wireless propagation channel. The received radio signal
is affected mainly by distance-dependent path-loss, shadowing due
to obstacles in the propagation medium, and small-scale fading due
to multipath effects. The received power 
can be expressed as

where  is a constant that captures antenna and other propagation
gains,  is the path-loss exponent, 
is the location-dependent shadowing and  is the
small-scale fading. We assume measurements average
(If measurements cannot average over small-scale fading, the proposed
framework from this paper cannot be applied.
) small-scale fading, either in time (measurements taken over a time
window), frequency (measurements represent average power over a large
frequency band), or space (measurements taken over multiple antennas)
. Therefore, the resulting
received signal power from the source node to a receiver node 
can be expressed in dB scale as

where  with 
and . A
common choice for modeling shadowing in wireless systems is through
a log-normal distribution, i.e., ,
where  is the shadowing variance. Shadowing 
is spatially correlated, with well-established correlation models
, among which the Gudmundson model
is widely used. Let  be
the scalar
(Vector measurements are also possible (e.g., from multiple base stations),
but not considered here for the sake of clarity.
) observation of the received power at node , which is written
as  where 
is a zero mean additive white Gaussian noise with variance .
For the sake of notational simplicity, we do not consider a three-dimensional
layout, the impact of non-uniform antenna gain patterns, or distance-dependent
path-loss exponents.


Location Error Model

In practice, nodes may not have access to their true location ,
but only to a distribution ( is used for  for
notational simplicity.). The distribution  is obtained
from the positioning algorithm in the devices, and depends on the
specific positioning technology (e.g., for GPS the distribution 
can be modeled as a Gaussian). We will assume that all distributions
 come from a given family of distributions (e.g.,
all bivariate Gaussian distributions). These distributions can be
described by a finite set of parameters, ,
, e.g., a mean and a covariance matrix for Gaussian distributions.
The set of descriptions of all distributions from the given family
is denoted by . Within this set,
the set of all delta Dirac distributions over locations is denoted
by . Note that  is equivalent
to the set  of possible locations. Finally, we introduce
a function  that extracts a position
estimate from the distribution (in our case chosen as the mean), and
denote . We will
generally make no distinction between a distribution 
and its representation .


Problem Statement

We assume a central coordinator, which collects a set of received
power measurements  with respect
to a common source from  nodes, along with their corresponding
location distributions .
Our goals are to perform

Learning: construct a spatial model (through estimating model
parameters , to be defined later) of the received
power based on the measurements;
Prediction: determine the predictive distribution 
of the power in test locations  and the distribution
of the expected
(Here,  should be interpreted as
the expected received power, ,
where  is described by 
) received power, ,
for test location distributions .

We will consider two methods for learning and prediction: classical
GP (Section ), which ignores location uncertainty and
only considers , and uncertain
GP (Section ), which is a method that explicitly accounts
for location uncertainty. We introduce 
and 
as the collection of true and estimated locations respectively. A
high level comparison of cGP and uGP is shown in Fig. ,
where cGP operates on  and , while uGP operates
on  and .


cGP classical GP
uGP  uncertain GP
z
u

High-level comparison
between cGP and uGP. The inputs to cGP during learning are observations
 and estimates  of the (unobserved) actual
locations  where those observations have been taken.
 is obtained through a positioning system. The true locations
 are marked with a triangle and are generally different
from the estimated locations , marked with a blue and
red dot. During prediction, cGP predicts received power at an estimated
test location, . In contrast, uGP considers the distribution
of the locations , described by  (and depicted
by the red and blue circle), during learning. During prediction, uGP
utilizes the distribution  of the test location.
Note that the amount of uncertainty (radius of the circle) can change. 




Channel Prediction with Classical GP

We first present cGP under the assumption that all locations during
learning and prediction are known exactly, based on.
Later in this section, we will discuss the impact of location uncertainties
on cGP in learning/training and prediction/testing.


cGP without Location Uncertainty 

We designate  as the input variable,
and  as the output variable.
We model  as a GP with mean function
 and a positive
semidefinite covariance function ,
and we write

where  stands for a Gaussian process. The mean function
(Other ways of including the mean function in the model are possible,
such as to include it in the covariance structure, and transform the
prior model to a zero-mean GP prior. 
) is defined as ,
due to (). The covariance function is defined
as .
We will consider a class of covariance functions of the form:

where  for  and zero otherwise, , 
is the correlation distance of the shadowing, and 
captures any noise variance term that is not due to measurement noise
(more on this later). Setting  in (),
gives the exponential covariance function that is commonly used to
describe the covariance properties of shadowing,
and , gives the squared exponential covariance function that
will turn out to be useful in Section . Note
that the mean and covariance depend on

which may not be known a priori.


Learning

The objective during learning is to infer the model parameters 
from observations  of the received power at  known
locations . The resulting training database is thus .
Due to the GP model, the joint distribution of the  training observations
exhibits a Gaussian distribution

p(yX,) & =N(y;(X),K),

where 
is the mean vector and  is the covariance matrix of the
measured received powers, with entries .
The model parameters can be learned through maximum likelihood estimation,
given the training database , by minimizing
the negative log-likelihood function with respect to :

 & =-(p(yX,)).

The negative log-likelihood function is usually not convex and may
contain multiple local optima. Additional details on the learning
process are provided later. Once  is determined
from , the training process is complete.


Prediction

After learning, we can determine the predictive distribution of 
at a new and arbitrary test location , given the
training database  and .
We first form the joint distribution

where  is the  vector of cross-covariances
 between the received power at
 and at the training locations ,
and  is the prior variance
(i.e., the variance in the absence of measurements), given by .
Conditioning on the observations , we obtain
the Gaussian posterior distribution 
for the test location . The mean ()
and variance () of this distribution
turn out to be

P_RX(x_*)= & (x_*)+k_*^TK^-1(y-(X))

= & (x_*)+_i,j=1^N[K^-1]_ij(y_j-(x_j)) C(x_*,x_i)

= & (x_*)+_i=1^N_i C(x_*,x_i).

V_RX(x_*)= & k_**-k_*^TK^-1k_*

= & k_**-_i,j=1^N[K^-1]_ij C(x_*,x_i) C(x_*,x_j),

where .
In (),  corresponds
to the deterministic path-loss component at , which
is corrected by a term involving the database and the correlation
between the measurements at the training locations and the test location.
In (), we see that the prior variance
 is reduced by a term that accounts for the correlation of
nearby measurements.



Impact of location uncertainty
for a one-dimensional example: the red curve depicts the received
signal power  as a function of 
(or equivalently, the distance to the base station), while the markers
show  as a function of .
Training measurements are grouped into three regions: (+) corresponds
to high uncertainty, () corresponds to low uncertainty, and
(*) corresponds to medium uncertainty, respectively. The location
uncertainty results in output noise.




cGP with Location Uncertainty

Now let us consider the case when the nodes do not have access to
their true location , but only to a distribution
, which is described by .
Fig.  illustrates the impact of
location uncertainties assuming Gaussian location errors for a one-dimensional
example. The figure shows (in red) the true received power 
as a function of  as well as the measured power 
as a function of  for a discrete
number of values of , shown as markers. To clearly illustrate
the impact of different amounts on uncertainty on the position, we
have artificially created three regions: high location uncertainty
close to the transmitter, medium location uncertainty far away, and
low location uncertainty for intermediate distances. When there is
no location uncertainty (70 m until 140 m from the transmitter), ,
so ,
and hence the black dots coincide with the red curve. For medium and
high uncertainty,  can differ significantly from
, so the data point with coordinates 
can lie far away from the red curve, especially for high location
uncertainty (distances below 70 m). From Fig. 
it is clear that the input uncertainty manifests itself as output
noise, with a variance that grows with increasing location uncertainty(In fact, the output noise induced by location uncertainty will also
depend on the slope of  around ,
since a locally flat function will lead to less output noise than
a steep function, under the same location uncertainty.). This output noise must be accounted for in the model during learning
and prediction. When these uncertainties are ignored, both learning
and prediction will be of poor quality, as described below.


Learning from uncertain training locations

In this case, the training database 
comprises  locations  and power
measurements  at the
true (but unknown) locations . The measurements will
be of the form shown in Fig. .
The estimated model parameters  can take
two forms: (i) assign very short correlation distances ,
large , and small ,
as some seemingly nearby events will appear uncorrelated: or (ii)
assign larger correlation distances , smaller ,
and explain the measurements by assigning a higher value to 
. In the first case, correlations between
measurement cannot be exploited, so that during prediction, the posterior
mean will be close to the prior mean and the posterior variance will
be close to the prior variance. In the second case, predictions will
be better, as correlations can be exploited to reduce the posterior
variance. However, the model must explain different levels of input
uncertainty with a single covariance function, which can make no distinctions
between locations with low, medium, or high uncertainty. This will
lead to poor performance when location error statistics differ from
node to node.


Prediction at an uncertain test location

In the case where training locations are exactly known (i.e., ,
), we may want to predict the power at an uncertain test
location , made available to cGP in the form ,
while the true test location  is not known. This
scenario can occur when a mobile user relies on a low-quality localization
system and reports an erroneous location estimate to the base station.
The wrong location has impact on the predicted posterior distribution
since the predicted mean  will differ from the
correct mean . In addition, 
will contain erroneous entries: the -th entry will be too small
when 
and too large when .
This will affect both the posterior mean ()
and variance (). In the case were training
locations are also unknown, i.e., , and
, these effects are further exacerbated
by the improper learning of .


Channel Prediction with Uncertain GP 

In the previous section, we have argued that cGP is unable to learn
and predict properly when training or test locations are not known
exactly, especially when location error statistics are heterogeneous.
In this section, we explore several possibilities to explicitly incorporate
location uncertainty. We recall that  denotes the set
of all distributions over the locations in the environment ,
while  represents the delta Dirac
distributions over the positions and has a one-to-one mapping to .

We will describe three approaches. First, a Bayesian approach where
the uncertain input (i.e., the uncertain location) is marginalized,
leading to a non-Gaussian output (i.e., the received power) distribution.
Second, we derive a Gaussian approximation of the output distribution
through moment matching and detail the corresponding learning and
prediction expressions. From these expressions, the concepts of expected
mean function and expected covariance function naturally appear. Finally,
we discuss uncertain GP, which is a Gaussian process with input 
from input set  and output . We will relate these
three approaches in a unified view. For each approach, we detail the
quality of the solution and the computational complexity. We note
that other approaches exist, e.g., through linearizing the output
around the mean of the input,
but they are limited to mildly non-linear scenarios.


Bayesian Approach

In a Bayesian context, we learn and predict by integrating the respective
distributions over the uncertainty of the training and test locations.
As this method will involve Monte Carlo integration, we will refer
to it as Monte Carlo GP (MCGP).


Learning

Given the training database , the likelihood
function with uncertain training locations 
is obtained by integrating
(For the sake of notation, all integrals in this section are written
as indefinite integrals, however they should be understood as definite
integrals over appropriate sets.
)  over the random
training locations:

where . As there
is generally no closed-form expression for the integral (),
we resort to a Monte Carlo approach by drawing  i.i.d. samples
,  so that

p(yU,) & 1M_m=1^Mp(yX^(m),)

 & =1M_m=1^MN(y;(X^(m)),K^(m)),

where 
and .
Finally, an estimate of  can be found by minimizing
the negative log-likelihood function

 & =-(p(yU,)),

which has to be solved numerically.

This optimization involves
high computational complexity and possibly numerical instability (due
to the sum of exponentials). More importantly, a good estimate of
 can only be found if a sample 
is generated that is close to the true locations . Due
to the high dimensionality,
this is unlikely, even for large . Hence, ()
will lead to poor estimates of .


Prediction

Given the training database  and ,
we wish to determine 
for an uncertain test location with associated distribution ,
described by . The posterior predictive distribution

is obtained by integrating 
with respect to  and :

p(P_RX(u_*)U,y,,u_*)

=p(P_RX(x_*)X,y,,x_*) p(X) p(x_*)dXdx_*.

This integral is again analytically intractable. The Laplace approximation
was utilized in to solve (),
while here we again resort to a Monte Carlo method by drawing 
i.i.d. samples  and ,
so that

 & p(P_RX(u_*)U,y,,u_*)

 & 1M_m=1^Mp(P_RX(x_*^(m))X^(m),y,,x_*^(m))

 & =1M_m=1^MN(P_RX(x_*^(m));P_RX(x_*^(m)),V_RX(x_*^(m))).

As  increases, the approximate distribution will tend to the true
distribution. We refer to () and ()
as Monte Carlo GP (MCGP). From (), we can compute
the mean ()
and the variance ()
 as

P_RX^MC(u_*) & =1M_m=1^MP_RX(x_*^(m))

V_RX^MC(u_*) & =1M_m=1^M(P_RX(x_*^(m))-P_RX^MC(u_*))^2

 & +1M_m=1^MV_RX(x_*^(m)).



Prediction is numerically straightforward, though it involves the
inversion of an  matrix  for each of the
 samples . In the case training locations are
known, we can utilize cGP to obtain a good estimate of 
and efficiently and accurately compute 
and . When both training
and test locations are known, the above procedure reverts to cGP.


Gaussian Approximation

We have seen that while MCGP can account for location uncertainty
during prediction, it will fail to deliver adequate estimates of 
during learning (see Remark ).
To address this, we can modify 
from () using a Gaussian approximation
through moment matching. In addition, we can also form a Gaussian
approximation of 
for prediction. We will term this approach Gaussian approximation
GP (GAGP). The expressions that are obtained in the learning of GAGP,
namely the expectation of mean and covariance functions will be used
later in the design of uncertain GP (described in Section ).


Learning

Given the training database , the mean
of  is given by

E[yU,] & =y p(yX,) p(X)dXdy 

 & =(y p(yX,)dy) p(X)dX

 & =(X) p(X)dX

 & =(U),

where 
and .
The covariance matrix of 
can be expressed as

 & Cov[y,yU,]

 & =yy^T p(yX,) p(X)dXdy-(U)(U)^T

 & =(K+(X)(X)^T) p(X)dX-(U)(U)^T

 & =K_u+,

where 
in which

 and  is a diagonal matrix with entries

We will refer to  and 
 as the expected mean and expected covariance function.
We can now express the likelihood function as 
so that  can be estimated by minimizing the
negative log-likelihood function

 & =-(N(y;(U),K_u+)).



Learning in GAGP involves computation of the expected mean in ()
and (), as well as the expected covariance
function in (). These integrals are generally
again intractable, but there are cases where closed-form expression
exist. These will be discussed
in detail in Section . GAGP avoids the numerical
problems present in MCGP and will hence generally be able to provide
a good estimate of .


Prediction

Given the training database  and ,
we approximate the predictive distribution 
by a Gaussian with mean 
and variance . These
are given by

 & P_RX^GA(u_*)

 & =E[P_RX(u_*)U,y,,u_*]

 & =P_RX(x_*) p(X) p(x_*)dXdx_*

 & =(u_*)+_i=1^N_i C(x_*,x_i) p(X) p(x_*)dXdx_*.

Note that  is itself a function of all 's
and . Similarly 
is calculated as

 & V_RX^GA(u_*)

 & =E[P_RX^2(u_*)U,y,,u_*]-P_RX^GA(u_*)^2

 & =(V_RX(x_*)+P_RX(x_*)^2) p(X) p(x_*)dXdx_*

 & -P_RX^GA(u_*)^2.

Note that both  and 
are functions of  (see ()-()).

Prediction in GAGP requires complex integrals to be solved in ()-()
for which no general closed-form expressions are known. Hence, a reasonable
approach is to use GAGP to learn  and MCGP for
prediction.



In case training locations are known, i.e., ,
() reverts to

P_RX^GA(u_*) & =(u_*)+_i=1^N_iC(x_*,x_i) p(x_*)dx_*

and () becomes

 & V_RX^GA(u_*)

 & =k_**-_i,j=1^N[K^-1]_ijC(x_*,x_i) C(x_*,x_j) p(x_*)dx_*

 & +(x_*)^2 p(x_*)dx_*+2_i=1^N_i((x_*) C(x_*,x_i)

 & p(x_*)dx_*)+_i,j=1^N_i_jC(x_*,x_i) C(x_*,x_j) p(x_*)dx_*

 & -P_RX^GA(u_*)^2,

both of which can be computed in closed form, under some conditions,
when  is constant in .
When both  and ,
GAGP reverts to cGP.


Uncertain GP

While GAGP avoids the learning problems inherent to MCGP, prediction
is generally intractable. Hence, GAGP is not a fully coherent approach
to deal with location uncertainty. To address this, we consider a
new type of GP (uGP), which operates directly on the location
distributions, rather than on the locations. uGP involves a mean
function 
and a positive semidefinite covariance function ,
which considers as inputs  and as outputs
. In other words,

The mean function is given by ,
already introduced as the expected mean function in ().
However, for the mean function to be useful in a GP context, it should
be available in closed form. As in cGP, we have significant freedom
in our choice of covariance function. Apart from all technical conditions
on the covariance function as described in,
it is desirable to have a covariance function that (i) is available
in closed form; (ii) leads to decreasing correlation with increasing
input uncertainty (even when both inputs have same mean); (iii) can
account for varying amounts of input uncertainty; (iv) reverts to
a covariance function of the form ()
when , (v) does not depend on the mean
function . We will now describe the mean function
 and covariance function 
in detail.


The mean function

According to law of iterated expectations, the mean function 
is expressed as

 While there is no closed-form expression available for (),
we can form a polynomial approximation ,
where the coefficients  are found by least squares minimization.
For a given range of , this approximation
can be made arbitrarily close by increasing the order . When 
is approximately Gaussian (which may be the case for ),

can be evaluated in closed form, since all Gaussian moments are known.
See Appendix  for details on the
approximation.


The covariance function

While any covariance function meeting the criteria (i)-(v) listed
above can be chosen, a natural choice is (see Section )

C_uGP(u_i,u_j) & =Cov[P_RX(x_i),P_RX(x_j)u_i,u_j]

 & =Cov[y_i,y_jU,]-_ij_n^2.

Unfortunately, as we can see from (), this choice
does not satisfy criterion (v). An alternative choice is the expected
covariance function 
from (). This choice clearly satisfies
criteria (ii), (iii), (iv), and (v). To satisfy (i), we can select
appropriate covariance functions, tailored to the distributions ,
or appropriate distributions  for a given covariance
function. Examples include:

Polynomial covariance functions for Gaussian 
.
Covariance functions of the form ()
with , , for Laplacian .
Covariance functions of the form ()
with , , for Gaussian 
(i.e., ).
The expected covariance function is then given by

 & C_uGP(u_i,u_j)=C_u(u_i,u_j)=_ij_proc^2

 & +_^2I+d_c^-2(_i+_j)(1-_ij)^-1/2

 & (-1d_c^2(z_i-z_j)^T(I+d_c^-2(_i+_j))^-1(z_i-z_j)).

Note that the factor 
ensures that inputs  with the same mean (i.e., )
exhibit lower correlation with increasing uncertainty. The factor
 ensures
that the measurements taken at locations with low uncertainty (smaller
than ) can be explained by a large value of , while
for measurements taken at locations with high uncertainty, 
will be small and decreasing with increasing uncertainty.


Learning

Given the training database  and choosing
 and ,
the model parameters are found by minimizing the log-likelihood function

 & =-(p(yU,)

 & =-(N(y;(U),K_u).

Note that in contrast to GAGP, we have constructed uGP so that 
and  are available in closed form, making
numerical minimization tractable.

Learning of uGP () corresponds to the case
of learning () in GAGP for 
(e.g., for constant mean processes).


Prediction

Let  be the mean and 
be the variance of the posterior predictive distribution 
of uGP with uncertain training and test locations, then .
The expressions for  and 
are now in standard GP form:

P_RX(u_*) & =(u_*)+k_u*^TK_u^-1(y-(U))

V_RX(u_*) & =k_u**-k_u*^TK_u^-1k_u*,

where  is the  vector of cross-covariances
 between the received
power at the test distribution  and at the training
distribution , and  is the a priori
variance .

In case the training locations are known, i.e., ,
the mean  and the variance
 can be obtained from the expressions
() and (), respectively,
by setting .
Furthermore, the resulting mean 
is exactly the same as (), obtained
in GAGP. However, due to a different choice of covariance function,
the predicted variance  is different
from ().



When the test location is known, i.e., ,
the mean  and the variance
 are obtained from ()
and () by setting .

a
b
c
d
e
f
g
h
j
i

mLearn
nLearn
oPredict
pPredict
lcGP
kuGP
qDatabase
r

Learning and prediction phases of
cGP and uGP. The difference in learning in uGP compared to cGP is
that it considers location uncertainty of the nodes. The estimated
model parameters  are derived during the
learning phase and are generally different in cGP compared to uGP.
The mean  and variance 
of the posterior predictive distribution in cGP corresponds to a location
 extracted from , which in turn
represents . In contrast, the mean 
and variance  of the posterior predictive
distribution in uGP pertains to the entire location distribution represented
by .


U
X
N
NGaussian output dist.
P
Pall output dist.

text1cGP
text2
text3MCGP
text4GA
text5GAGP
text6uGP
inputinput set
outputoutput set

Relation between cGP, MCGP, GAGP,
and uGP. All methods are equivalent when the input is limited to 
(grey shaded area). 




Unified View

We are now ready to recap the main differences between cGP and uGP,
and to provide a unified view of the four methods (cGP, MCGP, GAGP,
and uGP). Fig.  describes the main
processes in uGP and cGP, along with the inputs and outputs during
the learning and prediction processes. The four methods are depicted
in Fig. : all four methods revert
to cGP when training and predictions occur in , i.e.,
when there is no uncertainty about the locations. MCGP is able to
consider general input distributions in , but leads
to non-Gaussian output distributions. Through a Gaussian approximation
of these output distributions, GAGP can consider general inputs and
directly determine a Gaussian output distribution. Both of these approaches
(MCGP and GAGP) have in common that they treat the process with input
 as a GP. In contrast, uGP treats the process
with input  as a GP. This allows for a
direct mapping from inputs in  to Gaussian output distributions.
In terms of tractability for learning and prediction, the four methods
are compared in Table . We see that
among all four methods, uGP combines tractability with good performance.


Comparison of tractability for
cGP, MCGP, GAGP, and uGP in learning and prediction. 









Numerical Results and Discussion 

In this section, we show learning and prediction results of cGP, uGP,
and MCGP with uncertainty in training or test locations. In Section
, we describe a resource allocation
problem, where communication rates are predicted at future locations
using cGP and uGP, in the presence of location uncertainty during
training. The numerical analysis carried in this section is based
on simulated channel measurements according to the model outlined
in Section .


Simulation Parameters








Simulation Setup

A geographical region  is considered and a base station
is placed at the origin. A one dimensional radio propagation field
is generated with sampling locations at a resolution of 0.25 m using
an exponential covariance function ,
corresponding to the Gudmundson model. Small-scale fading is assumed
to have been averaged out(In the case small-scale fading is not averaged out, the proposed framework
cannot be applied. 
).The simulation parameters used to obtain the numerical
results are given in Table . We
assume isotropic localization errors, so that .
To capture the effect of heterogeneous location errors, we draw the
location error standard deviations from an exponential distribution,
i.e., ,
where  is the average location error standard deviation.
For cGP and MCGP, in order to not provide any unfair advantage to
uGP, we use a covariance function of the form ()
with , in order to match the true covariance function .
For uGP, we use (). Since uGP exhibits a
mismatch in the covariance function, we absorb this mismatch in ,
which is learned offline (more on this in Appendix ).
We assume nodes know  and , which be inferred
using standard methods,
so they are not included in the learning process.


Learning Under Location Uncertainty

Fig.  depicts the impact
of location uncertainty on the learning of hyperparameters 
for cGP, uGP, and MCGP. The learning of the hyperparameters is detailed
in Appendix .
*[tbh]

[]

centering

[]

centering


centering


[]

centering

[]

centering


centering



centering

Impact of location uncertainty
on learning the hyperparameters using cGP, uGP, and MCGP. The hyperparameters
are estimated for each value of the mean location error standard deviation
and for 40 realizations of the channel field. Results shown are the
mean estimate of the hyperparameters and error bars with one standard
deviation. Impact of location uncertainty in shown when estimating:
(a) , (b) , (c) ,
(d) . 




cGP

We first consider a variant of cGP, denoted as cGP-no-proc, in which
 is fixed to zero. In cGP-no-proc, when ,
the estimate  is non-zero. However, it can be observed
in Fig.  (a), that with
increase in ,  decreases quickly to zero.
Hence, cGP-no-proc will model the GP as a white process with high
variance  and thus cannot handle the location
uncertainty. On the other hand, in cGP where we estimate ,
 absorbs part of location uncertainty
(see Fig.  (c)). Consequently,
the part of the observations that must be explained through 
is reduced, leading to a reduction of  with
. Due to this, cGP considers the measurements constitute
a slowly varying process, therefore  increases with
. An interesting observation is that the error bars for
 also increase with . Hence, among cGP-no-proc
and cGP, only cGP can reasonably deal with location uncertainty.
*[tbh]

[]

centering

[]

centering


centering

Performance comparison of cGP, MCGP, and
uGP under uncertain training and certain testing locations. Inset
(a) received power prediction using uncertain training locations with
average location error of  = 8 m and certain test locations
for single realization of a channel field. The shaded area (grey for
cGP and blue for uGP) depicts point wise predictive mean plus and
minus the predictive standard deviation, and (b) MSE performance of
cGP and uGP as a function of average location error standard deviation
. The MSE is averaged for each value of  and for
50 realizations of the channel field is shown are the mean of the
MSE and error bars with one standard deviation. The MSE is calculated
as ,
where  is the set of test locations and 
denotes its cardinality.




MCGP

The behavior is similar to that of cGP, i.e., an increase in ,
and a decrease in , when increasing .
However,  decreases more quickly with 
when compared to cGP. These effects can be attributed to two causes:
first of all, the inherent problem of drawing a finite number of samples
as detailed at the end of Section ; secondly,
the fluctuations in the estimated path loss exponent 
with increasing  (see Fig. 
(d)). The error bars of the estimates in this case are even higher
than in cGP. As expected, MCGP is not suitable for learning.


uGP

As mentioned before, in uGP  is determined
offline. The uGP model has the capability to absorb the location uncertainty
into the covariance function. Due to this flexibility, it can handle
higher values of  and still maintain an almost constant
 and  with increase in .
For fair comparison with cGP, we also consider the case where 
is estimated as part of the learning, referred to as uGP-proc. It
can be observed in Fig. 
(c) that  increases with increase in
. When comparing uGP-proc to uGP, we observe a lower value
of  and higher values of  and
 for a particular value of .
From this, we conclude that uGP should be preferred over uGP-proc,
as it can explain the observations with smaller 
and leads to simpler optimization. Finally, note that the error bars
of the uGP estimates are relatively small when compared to cGP.


Prediction Under Location Uncertainty

Four cases can be considered, depending on whether training or testing
inputs are in  or . We will focus on the
case where either training or test locations are uncertain,
but not both. From these, the behavior when both training and testing
inputs are in  can be easily understood: only uGP can
give reasonable performance among cGP, MCGP, and uGP, as the estimates
of  in cGP and MCGP are of poor quality.


Uncertain training locations and certain testing locations

In this case  and .
Fig.  (a) depicts the prediction results
in terms of the predictive mean and predictive standard deviation
(shown as shaded areas) for a particular realization of the channel
field. It can be observed that uGP is able to predict the received
power comparatively better than cGP and MCGP. uGP is able to estimate
the underlying channel parameters better with the expected covariance
function, which takes in to account the location uncertainty of the
nodes. In turn, this means that uGP can track the faster variations
in the channel. cGP tries to model the true function with a slow varying
process due to very high . Furthermore, cGP has higher
uncertainty in predictions due to high 
(see Fig.  (c)). On the
other hand, MCGP has slightly better prediction performance (the standard
deviation is not shown, but is slightly smaller than for cGP) compared
to cGP due to the averaging by drawing samples from the distribution
of the uncertain training locations. Averaging the prediction error
over multiple channel realizations, Fig. 
(b) shows the mean squared error (MSE) of the received power prediction
of cGP and uGP with respect to  (MCGP is not shown due to
its similar performance to cGP). uGP clearly outperforms cGP (except
fo ) due to its better tracking of the true channel (see
Fig.  (a)) despite uncertainty on the training
locations. The reason for higher MSE in the case of  for
uGP is due to its kernel mismatch.


Certain training locations and uncertain testing locations

In this case  and 
(with a constant location error standard deviation  m). Now
the performance must be assessed with respect to the expected received
power ,
where ,
in which  is the mean of distribution described by
. An example is shown in Fig. 
(a), depicting  as a function of ,
as well as the predictions from cGP, MCGP, and uGP. It can be observed
that uGP and MCGP follow well . Specifically,
MCGP tracks  quite closely as it is
near-optimal in this case. In contrast, cGP follows the actual received
power at , rather than the averaged power. This leads
to fast variations in cGP, which are not present in uGP and MCGP.
Fig.  (b) shows the MSE of the received
power prediction of cGP, MCGP, and uGP with respect to  when
averaging the prediction error over multiple channel realizations.
As expected, MCGP has the lower MSE than uGP and cGP. However, uGP
performs better than cGP for all considered , except 
(due to kernel mismatch). Furthermore, the performance of uGP is very
close to that of MCGP.
*[tbh]

[]

centering

[]

centering


centering

Performance comparison of cGP, MCGP, and
uGP under certain training and uncertain testing locations. Inset
(a) received power prediction using certain training and uncertain
test locations with a constant location error standard deviation 
m for single realization of channel field, and (b) MSE performance
of cGP, MCGP and uGP as a function of constant location error standard
deviation  on test locations. The MSE is averaged for each
value of  and for 50 realizations of the channel field is
shown are the mean of the MSE and error bars with one standard deviation.
The MSE is calculated as ,
where  is the set of test location distributions
and  denotes its cardinality. 




Resource Allocation Example


Scenario

In this section, we compare cGP and uGP for a simple proactive resource
allocation scenario. We consider a user moving through a region 
and predict the CQM at each location. The supported rate, expressed
in bits per channel use (bpu), for a user at location 
is defined as

where ,
is the signal-to-noise ratio at location , 
is the receiver thermal noise and 
is the received power, both measured in linear scale. The average
rate in the region , denoted as ,
is defined as

where  denotes area of the region .
The predicted rate for a user at a future location ,
based on the predicted CQM values ,
is defined as

where  is a confidence parameter, 
and 
.


Performance measure

The user moves through the environment according to a known trajectory.
The base station allocates bits to each future location, proportional
to . When the user is at location ,
only a fraction of the bits, proportional to 
would be delivered. Therefore, the effective rate 
for the user at location  is
*[tbh]

[]

centering

[]

centering


centering

Resource allocation example
for cGP, and uGP with two different values of localization error standard
deviations ( m) and for different values of the
confidence parameter . The results are averaged for each
value of  with 50 channel realizations. Inset (a) the effective
rate , and (b) the
fraction of undelivered bits .


r^eff(x_*,) & =(r(x_*,),r(x_*)).

The average effective rate 
for a given confidence level  is then computed by spatial
average of  over region
 as

r_A^eff() & =1A_A r^eff(x_*,)dx_*[0,r_A^ref].

When , a part of the
allocated bits cannot be delivered. The total fraction of undelivered
bits over the environment is given by

Hence,  describes the
rate that the user will receive (penalizing under-estimation of the
rate), while  describes the loss due to lost bits (penalizing
over-estimating of the rate).


Predicted communication rates with uncertain training locations

We predict the CQM at known test locations ,
based on training with uncertain locations (considering 
m), all within a one-dimensional region . The average
effective rate  and
the fraction of undelivered bits , as a function of ,
are shown in Fig  (a)-(b),
respectively. As expected, increasing  leads to a more conservative
allocation, thus reducing both 
and . For a specific value of , increase in 
decreases . This is
due to the fact that with increase in , the mean 
is of poor quality and the variance 
is high for CQM predictions.

It is evident that when , uGP and cGP attain similar performance,
both in terms of  and
. When  is increased to 10 m, cGP suffers from
a significant reduction in effective rate ,
while at the same time dropping up to 4.5  of the bits. This is
due to cGP's poor predictions, which are either too low (leading to
a reduction in ) or
too high (leading to an increase in ). In contrast, uGP,
which is able to track the channel well despite uncertain training,
achieves a higher effective rate, especially for high confidence values
(e.g., around 2 times higher rate for , for 
less than 0.1).


Conclusion 

Channel quality metrics can be predicted using spatial regression
tools such as Gaussian processes (GP). We have studied the impact
of location uncertainties on GP and have demonstrated that, when heterogeneous
location uncertainties are present, the classical GP framework is
unable to (i) learn the underlying channel parameters properly; (ii)
predict the expected channel quality metric. By introducing a GP that
operates directly on the location distribution, we find uncertain
GP (uGP), which is able to both learn and predict in the presence
of location uncertainties. This translates in better performance when
using uGP for predictive resource allocation.

Possible avenues of future research include validation using real
measurements, modeling correlation of shadowing in the temporal dimension,
study of better approximations for learning with uncertain locations,
and the extension to ad-hoc networks.





Approximation of Expected Mean Function

Let  and recall from random variable
transformation theory that

We assume ,
so  follows a Rician distribution

where  is a modified Bessel function of zero-th order.
For ,  can be
approximated as a Gaussian distribution

The integral () still does not have a closed
form expression with . Now approximating
the  function with a polynomial function of the form
 then ()
can be written as

which can be computed exactly.


Learning Procedure

In this appendix, we detail the learning of 
for cGP, uGP, and MCGP. We consider nodes know  and ,
therefore they are not estimated as part of the learning process.
Let the remaining set of hyperparameters be 
and  .


cGP

Based on Section , we can write the received
measurements  with their corresponding training locations
 in matrix form as

where ,
 and .
Assuming the measurements are uncorrelated, then the least squares
estimate of the path-loss exponent can be computed as

Once the path-loss exponent is estimated, the mean component of the
received measurements can be subtracted as, .
Then,  becomes a zero-mean Gaussian
process. Now the likelihood function () becomes
.
The hyperparameters  are estimated by minimizing
negative logarithm of 

 & =-(p(_cX,)

 & =K+_c^TK^-1_c.

We calculate the variance of the process 
as .
The variance of the process should be captured by the hyperparameters
, , and . We
define ,
as a result  becomes a function of only 
and . We solve () and find
 and  by an exhaustive grid search.
Once  and  are found, then 
can be calculated as .


uGP

In this case, the path-loss exponent is estimated as

where .
Once again removing the mean from the measurements, we obtain 
The hyperparameters  are estimated by minimizing
the modified negative log-likelihood function

 & =-(p(_uU,)

 & =K_u+_u^TK_u^-1_u.

Again, ,
is the variance of the process. As a result, 
becomes 
and due to this  is now only a function of
. We solve () and find 
by an exhaustive grid search.

The learning process can be simplified for uGP: since 
only captures kernel mismatch irrespective of the location uncertainty
and path loss, the value of  can be
obtained off-line with noise-free training locations by performing
learning as in the case of cGP, but with a covariance function of
the form () for . This approach
gives an advantage to cGP and thus makes the comparison between uGP
and cGP more fair for all values of .


MCGP

It is no longer feasible to estimate  first and subtract to
make the process zero mean, because of summation in the Monte Carlo
integration (). Therefore, we optimize
() with respect to the hyperparameters 
and  using  function of
Matlab.


Acknowledgment

The authors would like to thank Ido Nevat, Lennart Svensson, Ilaria
Malanchini, and Vinay Suryaprakash for their feedback on the manuscript.

IEEEtran

10
[1]#1
url@samestyle

[2]#2
4
plus
3minus
  4
[2]
l@#1
** WARNING: IEEEtran.bst: No hyphenation pattern has been
** loaded for the language '#1'. Using the pattern for
** the default language instead.

l@#1

#2




S. Sand, R. Tanbourgi, C. Mensing, and R. Raulefs, "Position aware adaptive
  communication systems," in Forty-Third Asilomar Conference on Signals,
  Systems and Computers, 2009, pp. 73-77.


R. Di Taranto, S. Muppirisetty, R. Raulefs, D. Slock, T. Svensson, and
  H. Wymeersch, "Location-aware communications for 5G networks," IEEE
  Signal Processing Magazine, vol. 31, no. 6, pp. 102-112, Nov 2014.


H. Abou-zeid, H. Hassanein, and S. Valentin, "Optimal predictive resource
  allocation: Exploiting mobility patterns and radio maps," in IEEE
  Globecom Workshops, Dec 2013, pp. 4877-4882.


J. Johansson, W. Hapsari, S. Kelley, and G. Bodog, "Minimization of drive
  tests in 3GPP release 11," IEEE Communications Magazine, vol. 50,
  no. 11, pp. 36-43, 2012.


A. Zalonis, N. Dimitriou, A. Polydoros, J. Nasreddine, and P. Mahonen,
  "Femtocell downlink power control based on radio environment maps," in
  IEEE Wireless Communications and Networking Conference, April 2012,
  pp. 1224-1228.


A. Galindo-Serrano, B. Sayrac, S. Ben Jemaa, J. Riihijarvi, and P. Mahonen,
  "Harvesting MDT data: Radio environment maps for coverage analysis in
  cellular networks," in International Conference on Cognitive Radio
  Oriented Wireless Networks, July 2013, pp. 37-42.


I. Nevat, G. Peters, and I. Collings, "Location-aware cooperative spectrum
  sensing via Gaussian processes," in Australian Communications Theory
  Workshop, Jan 2012, pp. 19-24.


J. Tadrous, A. Eryilmaz, and H. El Gamal, "Proactive resource allocation:
  Harnessing the diversity and multicast gains," IEEE Transactions on
  Information Theory, vol. 59, no. 8, pp. 4833-4854, Aug 2013.


A. Goldsmith, Wireless communications.1em plus 0.5em minus
  0.4emCambridge university press, 2005.


N. Jalden, P. Zetterberg, B. Ottersten, A. Hong, and R. Thoma, "Correlation
  properties of large scale fading based on indoor measurements," in
  IEEE Wireless Communications and Networking Conference, March 2007,
  pp. 1894-1899.


M. S. Grewal, L. R. Weill, and A. P. Andrews, Global positioning systems,
  inertial navigation, and integration.1em plus 0.5em minus 0.4em
  John Wiley  Sons, 2001.


C. Rasmussen and C. Williams, Gaussian processes for machine
  learning.1em plus 0.5em minus 0.4emMIT Press, 2006.


L. Csato and M. Opper, "Sparse on-line Gaussian processes,"
  Neural computation, vol. 14, no. 3, pp. 641-668, 2002.


J. Quinonero-Candela and C. E. Rasmussen, "A unifying view of sparse
  approximate Gaussian process regression," The Journal of Machine
  Learning Research, vol. 6, pp. 1939-1959, 2005.


E. Snelson and Z. Ghahramani, "Sparse Gaussian processes using
  pseudo-inputs," Advances in neural information processing systems,
  vol. 18, p. 1257, 2006.


S. Sarkka, A. Solin, and J. Hartikainen, "Spatiotemporal learning via
  infinite-dimensional bayesian filtering and smoothing: A look at Gaussian
  process regression through Kalman filtering," IEEE Signal Processing
  Magazine, vol. 30, no. 4, pp. 51-61, 2013.


P. Dallaire, C. Besse, and B. Chaib-draa, "An approximate inference with
  Gaussian process to latent functions from uncertain data,"
  Neurocomputing, vol. 74, no. 11, pp. 1945-1955, 2011.


A. Girard, "Approximate methods for propagation of uncertainty with Gaussian
  process models," Ph.D. dissertation, University of Glasgow, 2004.


A. Girard and R. Murray-Smith, "Learning a Gaussian process model with
  uncertain inputs," Department of Computing Science, University of Glasgow,
  Tech. Rep. TR-2003-144, June 2003.


M. Jadaliha, Y. Xu, J. Choi, N. Johnson, and W. Li, "Gaussian process
  regression for sensor networks under localization uncertainty," IEEE
  Transactions on Signal Processing, vol. 61, no. 2, pp. 223-237, 2013.


A. McHutchon and C. E. Rasmussen, "Gaussian process training with input
  noise," in Advances in Neural Information Processing Systems, 2011,
  pp. 1341-1349.


A. Ranganathan, M.-H. Yang, and J. Ho, "Online sparse Gaussian process
  regression and its applications," IEEE Transactions on Image
  Processing, vol. 20, no. 2, pp. 391-404, Feb 2011.


S.-J. Kim, E. Dall'Anese, and G. Giannakis, "Cooperative spectrum sensing for
  cognitive radios using kriged Kalman filtering," IEEE Journal of
  Selected Topics in Signal Processing, vol. 5, no. 1, pp. 24-36, 2011.


D. Gu and H. Hu, "Spatial Gaussian process regression with mobile sensor
  networks," Neural Networks and Learning Systems, IEEE Transactions
  on, vol. 23, no. 8, pp. 1279-1290, Aug 2012.


M. P. Deisenroth and J. W. Ng, "Distributed Gaussian processes," arXiv
  preprint arXiv:1502.02843, 2015.


S. Choi, M. Jadaliha, J. Choi, and S. Oh, "Distributed Gaussian process
  regression for mobile sensor networks under localization uncertainty," in
  52nd Annual Conference on Decision and Control, Dec 2013, pp.
  4766-4771.


J. Quinonero Candela, "Learning with uncertainty-Gaussian processes and
  relevance vector machines," Ph.D. dissertation, Technical University of
  Denmark, 2004.


J. Fink, "Communication for teams of networked robots," Ph.D. dissertation,
  Elect. Syst. Eng., Univ. Pennsylvania, Philadelphia, PA, Aug 2011.


P. Agrawal and N. Patwari, "Correlated link shadow fading in multi-hop
  wireless networks," IEEE Transactions on Wireless Communications,
  vol. 8, no. 8, pp. 4024-4036, 2009.


M. Malmirchegini and Y. Mostofi, "On the spatial predictability of
  communication channels," IEEE Transactions on Wireless
  Communications, vol. 11, no. 3, pp. 964-978, 2012.


Y. Yan and Y. Mostofi, "Impact of localization errors on wireless channel
  prediction in mobile robotic networks," in IEEE Globecom, Workshop on
  Wireless Networking for Unmanned Autonomous Vehicles, Dec. 2013.


G. L. Stuber, Principles of Mobile Communication (2nd Ed.).
  1em plus 0.5em minus 0.4emKluwer Academic  Publishers, 2001.


A. Goldsmith, L. Greenstein, and G. Foschini, "Error statistics of real-time
  power measurements in cellular channels with multipath and shadowing,"
  IEEE Transactions on Vehicular Technology, vol. 43, no. 3, pp.
  439-446, Aug 1994.


S. S. Szyszkowicz, H. Yanikomeroglu, and J. S. Thompson, "On the feasibility
  of wireless shadowing correlation models," IEEE Transactions on
  Vehicular Technology, vol. 59, no. 9, pp. 4222-4236, 2010.


M. Gudmundson, "Correlation model for shadow fading in mobile radio systems,"
  Electronics letters, vol. 27, no. 23, pp. 2145-2146, 1991.


Y. Mostofi, M. Malmirchegini, and A. Ghaffarkhah, "Estimation of communication
  signal strength in robotic networks," in IEEE International Conference
  on Robotics and Automation, 2010, pp. 1946-1951.


D. J. MacKay, Information theory, inference, and learning
  algorithms.1em plus 0.5em minus 0.4emCambridge University
  Press, 2003.


D. Koller and N. Friedman, Probabilistic graphical models: principles and
  techniques.1em plus 0.5em minus 0.4emMIT press, 2009.


K. V. Mardia and R. Marshall, "Maximum likelihood estimation of models for
  residual covariance in spatial regression," Biometrika, vol. 71,
  no. 1, pp. 135-146, 1984.


P. K. Kitanidis, "Statistical estimation of polynomial generalized covariance
  functions and hydrologic applications," Water Resources Research,
  vol. 19, no. 4, pp. 909-921, 1983.





