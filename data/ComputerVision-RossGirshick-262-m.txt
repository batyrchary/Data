

font=small
[sub]font=small



-.5ex126




Theorem



***


Panoptic Segmentation

 Alexander Kirillov Kaiming He Ross Girshick
 Carsten Rother Piotr Dollar

 Facebook AI Research (FAIR) HCI/IWR, Heidelberg University, Germany



We propose and study a novel panoptic segmentation (PS) task. Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we first propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. Second, we are working to introduce panoptic segmentation tracks at upcoming recognition challenges. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.


Introduction


[subfigure]aboveskip=0mm,belowskip=.4mm
0.495
  image0.495
  semantic segmentation

0.495
  instance segmentation0.495
  panoptic segmentationFor a given (fig:image) image, we show ground truth for: (fig:semantic) semantic segmentation (per-pixel class labels), (fig:instance) instance segmentation (per-object mask and class label), and (fig:panoptic) the proposed panoptic segmentation task (per-pixel class+instance labels). The PS task: (1) encompasses both stuff and thing classes, (2) uses a simple but general format, and (3) introduces a uniform evaluation metric for all classes. Panoptic segmentation generalizes both semantic and instance segmentation and we expect the unified task will present novel challenges and enable innovative new methods.


In the early days of computer vision, things - countable objects such as people, animals, tools - received the dominant share of attention. Questioning the wisdom of this trend, Adelson elevated the importance of studying systems that recognize stuff - amorphous regions of similar texture or material such as grass, sky, road. This dichotomy between stuff and things persists to this day, reflected in both the division of visual recognition tasks and in the specialized algorithms developed for stuff and thing tasks.

Studying stuff is most commonly formulated as a task known as semantic segmentation, see Figure . As stuff is amorphous and uncountable, this task is defined as simply assigning a class label to each pixel in an image (note that semantic segmentation treats thing classes as stuff). In contrast, studying things is typically formulated as the task of object detection or instance segmentation, where the goal is to detect each object and delineate it with a bounding box or segmentation mask, respectively, see Figure . While seemingly related, the datasets, details, and metrics for these two visual recognition tasks vary substantially.

The schism between semantic and instance segmentation has led to a parallel rift in the methods for these tasks. Stuff classifiers are usually built on fully convolutional nets with dilations while object detectors often use object proposals and are region-based. Overall algorithmic progress on these tasks has been incredible in the past decade, yet, something important may be overlooked by focussing on these tasks in isolation.

A natural question emerges: Can there be a reconciliation between stuff and things? And what is the most effective design of a unified vision system that generates rich and coherent scene segmentations? These questions are particularly important given their relevance in real-world applications, such as autonomous driving or augmented reality.

Interestingly, while semantic and instance segmentation dominate current work, in the pre-deep learning era there was interest in the joint task described using various names such as scene parsing, image parsing, or holistic scene understanding. Despite its practical relevance, this general direction is not currently popular, perhaps due to lack of appropriate metrics or recognition challenges.

In our work we aim to revive this direction. We propose a task that: (1) encompasses both stuff and thing classes, (2) uses a simple but general output format, and (3) introduces a uniform evaluation metric. To clearly disambiguate with previous work, we refer to the resulting task as panoptic segmentation (PS). The definition of 'panoptic' is "including everything visible in one view", in our context panoptic refers to a unified, global view of segmentation.

The task format we adopt for panoptic segmentation is simple: each pixel of an image must be assigned a semantic label and an instance id. Pixels with the same label and id belong to the same object; for stuff labels the instance id is ignored. See Figure  for a visualization. This format has been adopted previously, especially by methods that produce non-overlapping instance segmentations. We adopt it for our joint task that includes stuff and things.

A fundamental aspect of panoptic segmentation is the task metric used for evaluation. While numerous existing metrics are popular for either semantic or instance segmentation, these metrics are best suited either for stuff or things, respectively, but not both. We believe that the use of disjoint metrics is one of the primary reasons the community generally studies stuff and thing segmentation in isolation. To address this, we introduce the panoptic quality (PQ) metric in sec:metric. PQ is simple and informative and most importantly can be used to measure the performance for both stuff and things in a uniform manner. Our hope is that the proposed joint metric will aid in the broader adoption of the joint task.

The panoptic segmentation task encompasses both semantic and instance segmentation but introduces new algorithmic challenges. Unlike semantic segmentation, it requires differentiating individual object instances; this poses a challenge for fully convolutional nets. Unlike instance segmentation, object segments must be non-overlapping; this presents a challenge for region-based methods that operate on each object independently. Generating coherent image segmentations that resolve inconsistencies between stuff and things is an important step toward real-world uses.

As both the ground truth and algorithm format for PS must take on the same form, we can perform a detailed study of human performance on panoptic segmentation. This allows us to understand the PQ metric in more detail, including detailed breakdowns of recognition segmentation and stuff things performance. Moreover, measuring human PQ helps ground our understanding of machine performance. This is important as it will allow us to monitor performance saturations on various datasets for PS.

Finally we perform an initial study of machine performance for PS. To do so, we define a simple and likely suboptimal heuristic that combines the output of two independent systems for semantic and instance segmentation via a series of post-processing steps that merges their outputs (in essence, a sophisticated form of non-maximum suppression). Our heuristic establishes a baseline for PS and gives us insights into the main algorithmic challenges it presents.

We study both human and machine performance on three popular segmentation datasets that have both stuff and things annotations. This includes the Cityscapes, ADE20k, and Mapillary Vistas datasets. For each of these datasets, we obtained results of state-of-the-art methods directly from the challenge organizers. In the future we will extend our analysis to COCO on which stuff is being annotated. Together our results on these datasets form a solid foundation for the study of both human and machine performance on panoptic segmentation.

We are currently working with challenge organizers from the COCO, Vistas, and ADE20k datasets to feature a panoptic segmentation track. We believe including a PS track alongside existing instance and semantic segmentation tracks on these popular recognition datasets will help lead to a broader adoption of the proposed joint task.


Related Work

Novel datasets and tasks have played a key role throughout the history of computer vision. They help catalyze progress and enable breakthroughs in our field, and just as importantly, they help us measure and recognize the progress our community is making. For example, ImageNet helped drive the recent popularization of deep learning techniques for visual recognition and exemplifies the potential transformational power that datasets and tasks can have. Our goals for introducing the panoptic segmentation task are similar: to challenge our community, to drive research in novel directions, and to enable both expected and unexpected innovation. We review related tasks next.

Object detection tasks. Early work on face detection using ad-hoc datasets (,) helped popularize bounding-box object detection. Later, pedestrian detection datasets helped drive progress in the field. The PASCAL VOC dataset upgraded the task to a more diverse set of general object classes on more challenging images. More recently, the COCO dataset pushed detection towards the task of instance segmentation. By framing this task and providing a high-quality dataset, COCO helped define a new and exciting research direction and led to many recent breakthroughs in instance segmentation. Our general goals for panoptic segmentation are similar.

Semantic segmentation tasks. Semantic segmentation datasets have a rich history and helped drive key innovations (, fully convolutional nets were developed using). These datasets contain both stuff and thing classes, but don't distinguish individual object instances. Recently the field has seen numerous new segmentation datasets including Cityscapes, ADE20k, and Mapillary Vistas. These datasets actually support both semantic and instance segmentation, and each has opted to have a separate track for the two tasks. Importantly, they contain all of the information necessary for PS. In other words, the panoptic segmentation task can be bootstrapped on these datasets without any new data collection.

Multitask learning. With the success of deep learning for many visual recognition tasks, there has been substantial interest in multitask learning approaches that have broad competence and can solve multiple diverse vision problems in a single framework. , UberNet solves multiple low to high-level visual tasks, including object detection and semantic segmentation, using a single network. While there is significant interest in this area, we emphasize that panoptic segmentation is not a multitask problem but rather a single, unified view of image segmentation. Specifically, the multitask setting allows for independent and potentially inconsistent outputs for stuff and things, while PS requires a single coherent scene segmentation.

Joint segmentation tasks. In the pre-deep learning era, there was substantial interest in generating coherent scene interpretations. The seminal work on image parsing proposed a general bayesian framework to jointly model segmentation, detection, and recognition. Later, approaches based on graphical models studied consistent stuff and thing segmentation. While these methods shared a common motivation, there was no agreed upon task definition, and different output formats and varying evaluation metrics were used, including separate metrics for evaluating results on stuff and thing classes. In recent years this direction has become less popular, perhaps for these reasons.

In our work we aim to revive this general direction, but in contrast to earlier work, we focus on the task itself. Specifically, as discussed, PS: (1) addresses both stuff and thing classes, (2) uses a simple format, and (3) introduces a uniform metric for both stuff and things. Previous work on joint segmentation uses varying formats and disjoint metrics for evaluating stuff and things. Methods that generate non-overlapping instance segmentations use the same format as PS, but these methods typically only address thing classes. By addressing both stuff and things, using a simple format, and introducing a uniform metric, we hope to encourage broader adoption of the joint task.

Amodal segmentation task. In objects are annotated amodally: the full extent of each region is marked, not just the visible. Our work focuses on segmentation of all visible regions, but an extension of panoptic segmentation to the amodal setting is an interesting direction for future work.


Panoptic Segmentation Format
Task format. The format for panoptic segmentation is simple to define. Given a predetermined set of  semantic classes encoded by , the task requires a panoptic segmentation algorithm to map each pixel  of an image to a pair , where  represents the semantic class of pixel  and  represents its instance id. The 's group pixels of the same class into distinct segments. Ground truth annotations are encoded identically. Ambiguous or out-of-class pixels can be assigned a special void label; , not all pixels must have a semantic label.

Stuff and thing labels. The semantic label set consists of subsets  and , such that  and . These subsets correspond to stuff and thing labels, respectively. When a pixel is labeled with , its corresponding instance id  is irrelevant. That is, for stuff classes all pixels belong to the same instance (, the same sky). Otherwise, all pixels with the same  assignment, where , belong to the same instance (, the same car), and conversely, all pixels belonging to a single instance must have the same . The selection of which classes are stuff things is a design choice left to the creator of the dataset, just as in previous datasets.

Relationship to semantic segmentation. The PS task format is a strict generalization of the format for semantic segmentation. Indeed, both tasks require each pixel in an image to be assigned a semantic label. If the ground truth does not specify instances, or all classes are stuff, then the task formats are identical (although the task metrics differ). In addition, inclusion of thing classes, which may have multiple instances per image, differentiates the tasks.

Relationship to instance segmentation. The instance segmentation task requires a method to segment each object instance in an image. However, it allows overlapping segments, whereas the panoptic segmentation task permits only one semantic label and one instance id to be assigned to each pixel. Hence, for PS, no overlaps are possible by construction. In the next section we show that this difference plays an important role in performance evaluation.

Confidence scores. Like semantic segmentation, but unlike instance segmentation, we do not require confidence scores associated with each segment for PS. This makes the panoptic task symmetric with respect to humans and machines: both must generate the same type of image annotation. It also makes evaluating human performance for PS simple. This is in contrast to instance segmentation, which is not easily amenable to such a study as human annotators do not provide explicit confidence scores (though a single precision/recall point may be measured). We note that confidence scores give downstream systems more information, which can be useful, so it may still be desirable to have a PS algorithm generate confidence scores in certain settings.


Panoptic Segmentation Metric
In this section we introduce a new metric for panoptic segmentation. We begin by noting that existing metrics are specialized for either semantic or instance segmentation and cannot be used to evaluate the joint task involving both stuff and thing classes. Previous work on joint segmentation sidestepped this issue by evaluating stuff and thing performance using independent metrics (). However, this introduces challenges in algorithm development, makes comparisons more difficult, and hinders communication. We hope that introducing a unified metric for stuff and things will encourage the study of the unified task.

Before going into further details, we start by identifying the following desiderata for a suitable metric for PS:

Completeness. The metric should treat stuff and thing classes in a uniform way, capturing all aspects of the task.

Interpretability. We seek a metric with identifiable meaning that facilitates communication and understanding.

Simplicity. In addition, the metric should be simple to define and implement. This improves transparency and allows for easy reimplementation. Related to this, the metric should be efficient to compute to enable rapid evaluation.

Guided by these principles, we propose a new panoptic quality (PQ) metric. PQ measures the quality of a predicted panoptic segmentation relative to the ground truth. It involves two steps: (1) segment matching and (2) PQ computation given the matches. We describe each step next then return to a comparison to existing metrics.

Segment Matching
We specify that a predicted segment and a ground truth segment can match only if their intersection over union (IoU) is strictly greater than 0.5. This requirement, together with the non-overlapping property of a panoptic segmentation, gives a unique matching: there can be at most one predicted segment matched with each ground truth segment.
 Given a predicted and ground truth panoptic segmentation of an image, each ground truth segment can have at most one corresponding predicted segment with IoU strictly greater than 0.5 and vice verse.

Let  be a ground truth segment and  and  be two predicted segments. By definition,  (they do not overlap). Since , we get the following:
  (p_i, g) = p_i gp_i g p_i gg for  i 1,2 .
Summing over , and since  due to the fact that , we get:
 (p_1, g) +  (p_2, g) p_1 g + p_2 gg 1 .
Therefore, if , then  has to be smaller than 0.5. Reversing the role of  and  can be used to prove that only one ground truth segment can have IoU with a predicted segment strictly greater than 0.5.

The requirement that matches must have IoU greater than 0.5, which in turn yields the unique matching theorem, achieves two of our desired properties. First, it is simple and efficient as correspondences are unique and trivial to obtain. Second, it is interpretable and easy to understand (and does not require solving a complex matching problem as is commonly the case for these types of metrics).

Note that due to the uniqueness property, for IoU  0.5, any reasonable matching strategy (including greedy and optimal) will yield an identical matching. For smaller IoU other matching techniques would be required; however, in the experiments we will show that lower thresholds are unnecessary as matches with IoU  0.5 are rare in practice.

PQ Computation



Toy illustration of ground truth and predicted panoptic segmentations of an image. Pairs of segments of the same color have IoU larger than 0.5 and are therefore matched. We show how the segments for the person class are partitioned into true positives , false negatives , and false positives .



We calculate PQ for each class independently and average over classes. This makes PQ insensitive to class imbalance. For each class, the unique matching splits the predicted and ground truth segments into three sets:  true positives (), false positives (), and false negatives (), representing matched pairs of segments, unmatched predicted segments, and unmatched ground truth segments, respectively. An example is illustrated in Figure . Given these three sets, PQ is defined as:
 psqPQ = _(p, g)  IoU(p, g) + 12 + 12 .
PQ is intuitive after inspection:  is simply the average IoU of matched segments, while  is added to the denominator to penalize segments without matches. Note that all segments receive equal importance regardless of their area. Furthermore, if we multiply and divide PQ by the size of the  set, then PQ can be seen as the multiplication of a segmentation quality (SQ) term and a recognition quality (RQ) term:
 psq-seg-detPQ = _(p, g)  IoU(p, g)12_segmentation quality (SQ)  + 12  + 12 _recognition quality (RQ)  .
Written this way, RQ is the familiar  score widely used for quality estimation in detection settings. SQ is simply the average IoU of matched segments. We find the decomposition of PQ = SQ  RQ to provide insight for analysis. We note, however, that the two values are not independent since SQ is measured only over matched segments.

Our definition of PQ achieves our desiderata. It measures performance of all classes in a uniform way using a simple and interpretable formula. We conclude by discussing how we handle void regions and groups of instances.

Void labels. There are two sources of void labels in the ground truth: (a) out of class pixels and (b) ambiguous or unknown pixels. As often we cannot differentiate these two cases, we don't evaluate predictions for void pixels. Specifically: (1) during matching, all pixels in a predicted segment that are labeled as void in the ground truth are removed from the prediction and do not affect IoU computation, and (2) after matching, unmatched predicted segments that contain a fraction of void pixels over the matching threshold are removed and do not count as false positives. Finally, outputs may also contain void pixels; these do not affect evaluation.

Group labels. A common annotation practice is to use a group label instead of instance ids for adjacent instances of the same semantic class if accurate delineation of each instance is difficult. For computing PQ: (1) during matching, group regions are not used, and (2) after matching, unmatched predicted segments that contain a fraction of pixels from a group of the same class over the matching threshold are removed and do not count as false positives.

Comparison to Existing Metrics

We conclude by comparing PQ to existing metrics for semantic and instance segmentation.

Semantic segmentation metrics. Common metrics for semantic segmentation include pixel accuracy, mean accuracy, and IoU. These metrics are computed based only on pixel outputs/labels and completely ignore object-level labels. For example, IoU is the ratio between correctly predicted pixels and total number of pixels in either the prediction or ground truth for each class. As these metrics ignore instance labels, they are not well suited for evaluating thing classes. Finally, please note that IoU for semantic segmentation is distinct from our segmentation quality (SQ), which is computed as the average IoU over matched segments.

Instance segmentation metrics. The standard metric for instance segmentation is Average Precision (AP). AP requires each object segment to have a confidence score to estimate a precision/recall curve. Note that while confidence scores are quite natural for object detection, they are not used for semantic segmentation. Hence, AP cannot be used for measuring the output of semantic segmentation, or likewise of PS (see also the discussion of confidences in sec:task).

Panoptic quality. PQ treats all classes (stuff and things) in a uniform way. We note that while decomposing PQ into SQ and RQ is helpful with interpreting results, PQ is not a combination of semantic and instance segmentation metrics. Rather, SQ and RQ are computed for every class (stuff and things), and measure segmentation and recognition quality, respectively. PQ thus unifies evaluation over all classes. We support this claim with rigorous experimental evaluation of PQ in sec:machines, including comparisons to IoU and AP for semantic and instance segmentation, respectively.


Panoptic Segmentation Datasets

To our knowledge only three public datasets have both dense semantic and instance segmentation annotations: Cityscapes, ADE20k, and Mapillary Vistas. We use all three datasets for panoptic segmentation. In addition, in the future we will extend our analysis to COCO on which stuff is currently being annotated(In addition to stuff annotations being incomplete, COCO instance segmentations contain overlaps. We plan on collecting depth ordering for all pairs of overlapping instances in COCO to resolve these overlaps.).

Cityscapes has 5000 images (2975 train, 500 val, and 1525 test) of ego-centric driving scenarios in urban settings. It has dense pixel annotations (97 coverage) of 19 classes among which 8 have instance-level segmentations.

ADE20k has over 25k images (20k train, 2k val, 3k test) that are densely annotated with an open-dictionary label set. For the 2017 Places Challenge(7pt1emhttp://placeschallenge.csail.mit.edu), 100 thing and 50 stuff classes that cover 89 of all pixels are selected. We use this closed vocabulary in our study.

Mapillary Vistas has 25k street-view images (18k train, 2k val, 5k test) in a wide range of resolutions. The 'research edition' of the dataset is densely annotated (98 pixel coverage) with 28 stuff and 37 thing classes.


Human Performance Study
One advantage of panoptic segmentation is that it enables measuring human performance. Aside from this being interesting as an end in itself, human performance studies allow us to understand the task in detail, including details of our proposed metric and breakdowns of human performance along various axes. This gives us insight into intrinsic challenges posed by the task without biasing our analysis by algorithmic choices. Furthermore, human studies help ground machine performance (discussed in sec:machines) and allow us to calibrate our understanding of the task.



1.0
   

1.0
   Segmentation flaws. Images are zoomed and cropped. Top row (Vistas image): both annotators identify the object as a car, however, one splits the car into two cars. Bottom row (Cityscapes image): the segmentation is genuinely ambiguous.




4pt1.1

Human performance for stuff things. Panoptic, segmentation, and recognition quality (PQ, SQ, RQ) averaged over classes (PQ=SQRQ per class) are reported as percentages.  Perhaps surprisingly, we find that human performance on each dataset is relatively similar for both stuff and things.


Human annotations. To enable human performance analysis, dataset creators graciously supplied us with 30 doubly annotated images for Cityscapes, 64 for ADE20k, and 46 for Vistas. For Cityscapes and Vistas, the images are annotated independently by different annotators. ADE20k is annotated by a single well-trained annotator who labeled the same set of images with a gap of six months. To measure panoptic quality (PQ) for human annotators, we treat one annotation for each image as ground truth and the other as the prediction. Note that the PQ is symmetric the ground truth and prediction, so order is unimportant.

Human performance. First, Table  shows human performance on each dataset, along with the decomposition of PQ into segmentation quality (SQ) and recognition quality (RQ). As expected, humans are not perfect at this task, which is consistent with studies of annotation quality from. Visualizations of human segmentation and classification errors are shown in Figures  and , respectively.

We note that Table  establishes a measure of annotator agreement on each dataset, not an upper bound on human performance. We further emphasize that numbers are not comparable across datasets and should not be used to assess dataset quality. The number of classes, percent of annotated pixels, and scene complexity vary across datasets, each of which significantly impacts annotation difficulty.

Stuff things. PS requires segmentation of both stuff and things. In Table  we also show PQand PQwhich is the PQ averaged over stuff classes and thing classes, respectively. For Cityscapes and ADE20k human performance for stuff and things are close, on Vistas the gap is a bit larger. Overall, this implies stuff and things have similar difficulty, although thing classes are somewhat harder. In Figure  we show PQ for every class in each dataset, sorted by PQ. Observe that stuff and things classes distribute fairly evenly. This implies that the proposed metric strikes a good balance and, indeed, is successful at unifying the stuff and things segmentation tasks without either dominating the error.



1.0
   

1.0
   Classification flaws. Images are zoomed and cropped. Top row (ADE20k image): simple misclassification. Bottom row (Cityscapes image): the scene is extremely difficult, tram is the correct class for the segment. Many errors are difficult to resolve.




4pt1.1

Human performance scale, for small (S), medium (M) and large (L) objects. Scale plays a large role in determining human accuracy for panoptic segmentation. On large objects both SQ and RQ are above 80 on all datasets, while for small objects RQ drops precipitously. SQ for small objects is quite reasonable.


Small large objects. To analyze how PQ varies with object size we partition the datasets into small (S), medium (M), and large (L) objects by considering the smallest , middle , and largest  of objects in each dataset, respectively. In Table , we see that for large objects human performance for all datasets is quite good. For small objects, RQ drops significantly implying human annotators often have a hard time finding small objects. However, if a small object is found, it is segmented relatively well.

IoU threshold. By enforcing an overlap greater than 0.5 IoU, we are given a unique matching by Theorem  . However, is the 0.5 threshold reasonable? An alternate strategy is to use no threshold and perform the matching by solving a maximum weighted bipartite matching problem. The optimization will return a matching that maximizes the sum of IoUs of the matched segments. We perform the matching using this optimization and plot the cumulative density functions of the match overlaps in Figure . Less than 16 of the matches have IoU overlap less than 0.5, indicating that relaxing the threshold should have minor effect.

To verify this intuition, in Figure  we show PQ computed for different IoU thresholds. Notably, the difference in PQ for IoU of 0.25 and 0.5 is relatively small, especially compared to the gap between IoU of 0.5 and 0.75, where the change in PQ is larger. Furthermore, many matches at lower IoU are false matches. Therefore, given that the matching for IoU of 0.5 is not only unique, but also simple and intuitive, we believe that the default choice of 0.5 is reasonable.



Per-Class Human performance, sorted by PQ. Thing classes are shown in red, stuff classes in orange (for ADE20k every other class is shown, classes without matches in the dual-annotated tests sets are omitted). Things and stuff are distributed fairly evenly, implying PQ balances their performance.




Cumulative density functions of overlaps for matched segments in three datasets when matches are computed by solving a maximum weighted bipartite matching problem. After matching, less than 16 of matched objects have IoU below 0.5.




Human performance for different IoU thresholds. The difference in PQ using a matching threshold of 0.25 0.5 is relatively small. For IoU of 0.25 matching is obtained by solving a maximum weighted bipartite matching problem. For a threshold greater than 0.5 the matching is unique and much easier to obtain.


SQ RQ balance. Our RQ definition is equivalent to the  score. However, other choices are possible. Inspired by the generalized  score, we can introduce a parameter  that enables tuning the penalty for recognition errors:
 alphaRQ^=  +  +  .
By default  is 0.5. Lowering  reduces the penalty of unmatched segments and thus increases RQ (SQ is not affected). Since PQ=SQRQ, this changes the relative effect of PS RQ on the final PQ metric. In Figure  we show SQ and RQ for various . The default  strikes a good balance between SQ and RQ. In principle, altering  can be used to balance the influence of segmentation and recognition errors on the final metric. In a similar spirit, one could also add a parameter  to balance influence of FPs FNs.



SQ RQ for different , see (). Lowering  reduces the penalty of unmatched segments and thus increases the reported RQ (SQ is not affected). We use  of 0.5 throughout but by tuning  one can balance the influence of SQ and RQ in the final metric.



Machine Performance Baselines
We now present simple machine baselines for panoptic segmentation. We are interested in three questions: (1) How do heuristic combinations of top-performing instance and semantic segmentation systems perform on panoptic segmentation? (2) How does PQ compare to existing metrics like AP and IoU? (3) How do the machine results compare to the human results that we presented previously?

Algorithms and data. We want to understand panoptic segmentation in terms of existing well-established methods. Therefore, we create a basic PS system by applying reasonable heuristics (described shortly) to the output of existing top instance and semantic segmentation systems.

We obtained algorithm output for three datasets. For Cityscapes, we use the val set output generated by the current leading algorithms (PSPNet and Mask R-CNN for semantic and instance segmentation, respectively). For ADE20k, we received output for the winners of both the semantic and instance segmentation tracks on a 1k subset of test images from the 2017 Places Challenge. For Vistas, which is used for the LSUN'17 Segmentation Challenge, the organizers provide us with 1k test images and results from the winning entries for the instance and semantic segmentation tracks.

Using this data, we start by analyzing PQ for the instance and semantic segmentation tasks separately, and then examine the full panoptic segmentation task. Note that our 'baselines' are very powerful and that simpler baselines may be more reasonable for fair comparison in papers on PS.



4pt1.05

Machine results on instance segmentation (stuff classes ignored). Non-overlapping predictions are obtained using the proposed heuristic. APNO is AP of the non-overlapping predictions. As expected, removing overlaps harms AP as detectors benefit from predicting multiple overlapping hypotheses. Methods with better AP also have better APNO and likewise improved PQ.


Instance segmentation. Instance segmentation algorithms produce overlapping segments. To measure PQ, we must first resolve these overlaps. To do so we develop a simple non-maximum suppression (NMS)-like procedure. We first sort the predicted segments by their confidence scores and remove instances with low scores. Then, we iterate over sorted instances, starting from the most confident. For each instance we first remove pixels which have been assigned to previous segments, then, if a sufficient fraction of the segment remains, we accept the non-overlapping portion, otherwise we discard the entire segment. All thresholds are selected by grid search to optimize PQ. Results on Cityscapes and ADE20k are shown in Table  (Vistas is omitted as it only had one entry to the 2017 instance challenge). Most importantly, AP and PQ track closely, and we expect improvements in a detector's AP will also improve its PQ.

Semantic segmentation. Semantic segmentations have no overlapping segments by design, and therefore we can directly compute PQ. In Table  we compare mean IoU, a standard metric for this task, to PQ. For Cityscapes, the PQ gap between methods corresponds to the IoU gap. For ADE20k, the gap is much larger. This is because whereas IoU counts correctly predicted pixel, PQ operates at the level of instances. See the Table  caption for details.



6pt1.05

Machine results on semantic segmentation (thing classes ignored). Methods with better mean IoU also show better PQ results. Note that G-RMI has quite low PQ. We found this is because it hallucinates many small patches of classes not present in an image. While this only slightly affects IoU which counts pixel errors it severely degrades PQ which counts instance errors.


Panoptic segmentation. To produce algorithm outputs for PS, we start from the non-overlapping instance segments from the NMS-like procedure described previously. Then, we combine those segments with semantic segmentation results by resolving any overlap between thing and stuff classes in favor of the thing class (, a pixel with a thing and stuff label is assigned the thing label and its instance id). This heuristic is imperfect but sufficient as a baseline.

Table  compares PQand PQcomputed on the combined ('panoptic') results to the performance achieved from the separate predictions discussed above. For these results we use the winning entries from each respective competition for both the instance and semantic tasks. Since overlaps are resolved in favor of things, PQis constant while PQis slightly lower for the panoptic predictions. Visualizations of panoptic outputs are shown in Figure .

Human machine panoptic segmentation. To compare human machine PQ, we use the machine panoptic predictions described above. For human results, we use the dual-annotated images described in sec:human and use bootstrapping to obtain confidence intervals since these image sets are small. These comparisons are imperfect as they use different test images and are averaged over different classes (some classes without matches in the dual-annotated tests sets are omitted), but they can still give some useful signal.

We present the comparison in Table . For SQ, machines trail humans only slightly. On the other hand, machine RQ is dramatically lower than human RQ, especially on ADE20k and Vistas. This implies that recognition, , classification, is the main challenge for current methods. Overall, there is a significant gap between human and machine performance. We hope that this gap will inspire future research for the proposed panoptic segmentation task.


*
1.0 image
 cityscapes_82_img
 cityscapes_234_img
 ade_36_img
 ade_234_img
 ade_384_img


1.0 ground truth
 cityscapes_82_gt
 cityscapes_234_gt
 ade_36_gt
 ade_234_gt
 ade_384_gt


1.0 prediction
 cityscapes_82_pred
 cityscapes_234_pred
 ade_36_pred
 ade_234_pred
 ade_384_pred

Panoptic segmentation results on Cityscapes (left two) and ADE20k (right three). Predictions are based on the merged outputs of state-of-the-art instance and semantic segmentation algorithms (see Tables  and ). Colors for matched segments (IoU0.5) match (crosshatch pattern indicates unmatched regions and black indicates unlabeled regions). Best viewed in color and with zoom.





18pt1.05

Panoptic independent predictions. The 'machine-separate' rows show PQ of semantic and instance segmentation methods computed independently (see also Tables  and ). For 'machine-panoptic', we merge the non-overlapping thing and stuff predictions obtained from state-of-the-art methods into a true panoptic segmentation of the image. Due to the merging heuristic used, PQstays the same while PQis slightly degraded.



Future of Panoptic Segmentation

Our goal is to drive research in novel directions by inviting the community to explore the new panoptic segmentation task. We believe that the proposed task can lead to expected and unexpected innovations. We conclude by discussing some of these possibilities and our future plans.

Motivated by simplicity, the PS 'algorithm' in this paper is based on the heuristic combination of outputs from top-performing instance and semantic segmentation systems. This approach is a basic first step, but we expect more interesting algorithms to be introduced. Specifically, we hope to see PS drive innovation in at least two areas: (1) Deeply integrated end-to-end models that simultaneously address the dual stuff-and-thing nature of PS.  A number of instance segmentation approaches including are designed to produce non-overlapping instance predictions and could serve as the foundation of such a system. (2) Since a PS cannot have overlapping segments, some form of higher-level 'reasoning' may be beneficial, for example, based on extending learnable NMS to PS. We hope that the panoptic segmentation task will invigorate research in these areas leading to exciting new breakthroughs in vision.

Finally, we are working with competition organizers to extend popular segmentation datasets to include a panoptic segmentation  track. Currently the COCO, Vistas, and ADE20k challenges are considering featuring a panoptic segmentation track in 2018. We hope this will lead to a broad adoption of the proposed joint task.



6pt1.05

Human machine performance. On each of the considered datasets human performance is much higher than machine performance (approximate comparison, see text for details). This is especially true for RQ, while SQ is closer. The gap is largest on ADE20k and smallest on Cityscapes. Note that as only a small set of human annotations is available, we use bootstrapping and show the the 5th and 95th percentiles error ranges for human results.





1style.files/ieee

