2019
 



  
  
  
  
  

  
  


  
/Title ()
/Author (AAAI Press Staff)
  
 


Learning Disentangled Representations for Timber and Pitch
 in Music Audio







Yun-Ning Hung, Yi-An Chen and Yi-Hsuan Yang
 Research Center for IT Innovation, Academia Sinica, Taiwan 

 KKBOX Inc., Taiwan

biboamy,yang@citi.sinica.edu.tw, annchen@kkbox.com


Timbre and pitch are the two main perceptual properties of musical sounds. Depending on the target applications, we sometimes prefer to focus on one of them, while reducing the effect of the other. 
Researchers have managed to hand-craft such timbre-invariant or pitch-invariant features using domain knowledge and signal processing techniques, but it remains difficult to disentangle them in the resulting feature representations. 
Drawing upon state-of-the-art techniques in representation learning, we propose in this paper two deep convolutional neural network models for learning disentangled representation of musical timbre and pitch. 
Both models use encoders/decoders and adversarial training to learn music representations, but the second model additionally uses skip connections to deal with the pitch information. 
As music is an art of time,

the two models are supervised by frame-level instrument and pitch labels using a new dataset collected from MuseScore. 
We compare the result of the two disentangling models with a new evaluation protocol called "timbre crossover," which leads to interesting applications in audio-domain music editing. 
Via various objective evaluations, we show that the second model can better change the instrumentation of a multi-instrument music piece without much affecting the pitch structure. 

By disentangling timbre and pitch, we envision that the model can contribute to generating more realistic music audio as well.





Introduction
Timbre and pitch are the two main perceptual properties of musical sounds. For a musical note, they refer to the perception of sound quality and frequency of the note, respectively. For a musical phrase, the perception of pitch informs us the notes and their ordering in the phrase (e.g., Do-Do-So-So-La-La-So), whereas the perception of timbre informs us the instruments that play each note. 
Timbre and pitch are interdependent, but they can also be disentangled-we can use different instruments to play the same note sequence, and the same instrument to play different note sequences. 
While listening to music, human beings can selectively pay their attention to either the timbre or pitch aspect of music. 
For AI applications in music, we hope machines can do the same.


Preserving the characteristics of one property while reducing those of the other has been studied in the literature.
When the goal is to build a computational model that recognizes the note sequences (e.g., for tasks such as query by humming  and cover song identification ), we need a feature representation of music that is not sensitive to changes in timbre and instrumentation. In contrast, in building an instrument or singer classifier,
we may want to focus more on timbre rather than pitch.

The pursuit of such timbre- or pitch-invariant features has been mostly approached with domain knowledge and signal processing techniques. 
However, the effectiveness of these features is usually evaluated by their performance in the downstream recognition or classification problems, i.e., from an analysis point of view. 
For example, a timbre-invariant feature is supposed to work better than a non timbre-invariant one for harmonic analysis. 
It remains unclear how timbre and pitch are actually disentangled in the feature representations.

With the recent success in learning disentangled representations for images using deep autoencoders ,
we see new opportunities to tackle timbre and pitch disentanglement for music from the synthesis point of view.
Taking a musical audio clip as input, we aim to build a model that processes the timbre and pitch information in two different streams to arrive at the intermediate timbre and pitch representations that are disentangled, and that can be combined to reconstruct the original input. 
If timbre and pitch are successfully disentangled, we expect that we can change the instrumentation of a music clip 
with this model 
by manipulating the timbre representation only, while fixing the pitch representation.(If we think about timbre as tone colors, this is like coloring the music clip in different ways. When there are multiple instruments, the model needs to decide which instrument plays which notes.)

To our best knowledge, this work represents the first attempt to disentangle timber and pitch in music audio with deep encoder/decoder architectures. Our approach has a few advantages over the conventional analysis approach.
First, we adopt a deep neural network to learn features in a data-driven way, instead of hand-crafting the features.
Second, we can use the learned features to generate an audio signal, which provides a direct way to evaluate the effectiveness of disentanglement.
Third, accordingly, it enables new applications in audio-domain music editing-to manipulate the timbre and pitch content of an existing music clip without re-recording it.
Lastly, given a note sequence generated by human or an AI composer ,  
our model can help decide how to color (i.e., add timbre to) it.

Due to the differences in images and music, we cannot directly apply existing methods to music. 
Instead, we propose two ideas that consider the specific characteristics of music in designing the encoders and decoders.

The first idea is temporal supervision.
In computer vision, people use image-level attributes as the supervisory signal to learn disentangled features. For example, to disentangle face identities and poses , or to disentangle different attributes of faces .
For music, we cannot analogously use clip-level labels, since timbre and pitch are associated with each individual musical notes, and a music clip is composed of multiple notes. Therefore, we propose to use the multi-track pianorolls as the learning target of our encoders/decoders to provide detailed temporal supervision at the frame level.
That is, instead of aiming to reconstruct the input audio, we aim to generate as the output of the network the pianoroll associated with the input audio. 

A pianoroll is a symbolic representation of music that specifies the timbre and pitch per note. It can be derived from a MIDI file .
We manage to compile a new dataset with 350,000 pairs of audio clips and time-aligned MIDIs. 
The temporal supervision provided by the dataset greatly facilitates timbre and pitch disentanglement, for otherwise the model has to learn from the audio signals in an unsupervised way.
We intend to make public the dataset and our code for reproducibility.


The second idea is to use different operations to deal with timbre and pitch. Specifically, we propose to use convolutions in the encoder to learn the abstract timbre representation in the latent space, and use symmetric skip connections to allow the pitch information to flow directly from the encoder to the decoder. This design is based on the intuition that, when we use a time-frequency representation such as the spectrogram as the input, the timbre aspect actually affects how the energy of the harmonic partials distributes along the frequency axis and develops over time, whereas the pitch aspect determines only what the fundamental frequency and duration of the notes are.
To make an analogy between music and images, pitch acts like the boundary of visual objects, whereas timbre acts like the texture. We therefore aim to learn an embedding for the texture, while process the discrete pitch information with skip connections, which have been found effective in image segmentation.  

Specifically, we propose two models for disentangling timbre and pitch. The first model (DuoAE) uses separate encoders for timbre and pitch, whereas the second model (UnetAE) adopts the aforementioned second idea and use skip connections to deal with pitch. 
Both models employ adversarial training. 
Figure  illustrates the two models. We will present the model details later.






As secondary contributions, for music editing and generation purposes, we additionally train another encoder/decoder sub-network to convert the pianorolls into audio signals. We find that the use of binary neurons  is critical for this new sub-network. In addition, we propose a new evaluation protocol called "timbre crossover" to evaluate how well we can create a new music by exchanging the instrumentation of two existing pieces, using the timbre representation of one piece in reconstructing the pianoroll or audio of the other piece.

We report systematic objective evaluations of the result of timbre crossover.






















[b]0.5 
DuoAE: Use separate encoders/decoders for timbre and pitch

UnetAE: Use skip connections to process pitch

The two proposed encoder/decoder architectures for disentangling timbre and  pitch for music audio. The dashed lines indicate the adversarial training parts. (Notations: CQT-a time-frequency representation of audio; roll-multi-track pianoroll; E-encoder; D-decoder; Z-latent code; t-timbre; p-pitch; skip-skip connections).


Background



A deep autoencoder (or AE for short) is a network architecture that uses a stack of encoding layers (known collectively as an encoder) to get a low-dimensional representation of data, which originally resides in a high-dimensional space . The resulting representation is also known as the latent code. 
The network is trained such that we can recover the input from the latent code, by passing the latent code through another stack of decoding layers (known collectively as a decoder).
Compared to other representation learning methods, the AE has the advantages that the training is unsupervised, and that the obtained representation can be mapped back to the data space.

With the original AE, different properties of data might be entangled 
in the latent code, meaning that each entry of the latent code is related to multiple data properties.
For instance, we can train an AE with face images to obtain a latent vector  for an image, where  denotes the dimensionality of the latent code. If we add some noise  to only a random entry of  and then decode it, likely many properties of the decoded face 
would be different from the original face.


With some labeled data, we can train an AE in a way that different parts of the latent code correspond to different data properties.
For example, given face images labeled with identities and poses, we can divide  into two parts  and  (i.e., ), and use them as the input to two separate stacks of fully-connected layers to train an identity classifier  and a pose classifier , respectively. We still require the concatenated code (i.e., ) to reconstruct the input. With this AE, if we 
add changes to  but keep  the same, 

we may obtain an image with the same identity but a different pose. In this way, we call  and   disentangled representations of face identities and poses.


We can further improve the result of disentanglement by using adversarial training. As usual, we aim to minimize the classification error of  and  when their input is  and , respectively. However, we 
additionally use  and  as the input to  and  respectively, and aim to maximize the classification error of the two classifiers under such a scenario. In this way, we promote identity information in  while dispel any information related to pose, and similarly for . This idea was proposed by .

There are many other ways to achieve disentanglement, e.g., using generative adversarial networks (GAN) ,  
cross-covariance penalties ,
and latent space arithmetic operations . Some are unsupervised methods.
While the majority of work has been on images, feature disentanglement has also been studied for video clips , speech clips , 
3D data, and MIDIs . 

Our work distinguishes itself from the existing works mainly in the following two aspects. First, we use temporal supervision to learn disentangled representations, while existing work usually use image- or clip-level labels, such as face identity, speaker identity , and genre . 

Second, to our best knowledge, little has been done for disentangled representation learning in music audio.

Although MIDI is also a type of music, an disentangling model for encoding MIDIs cannot be applied to musical audio signals.(For example, the sounds of different partials from different instruments are mixed in audio, but this does not happen in MIDIs.)(Disentangling musical properties in MIDIs is a challenging task as well.  
 aimed to disentangle melody and instrumentation using a variational autoencoder (VAE), but they found that manipulating the instrumentation code (while fixing the melody code) would still change the melody.)


































Proposed Models for Disentanglement

Figure  shows the architecture of the proposed models for disentangling timbre and pitch. We present the details below.



Input/Output Data Representation

Input
The input to our models is an audio waveform with arbitrary length. To facilitate timbre and pitch analysis, we firstly convert the waveform into a time-frequency representation that shows the energy distribution across different frequency bins for each short-time frame. Instead of using the short-time Fourier transform (STFT), we use the constant-Q transform (CQT) here, for the latter adopts a logarithmic frequency scale that better aligns with our perception of pitch . CQT also provides better frequency resolution in the low-frequency part, which helps detect the fundamental frequencies.

As will be shown later, our encoders and decoders are designed to be fully-convolutional , so that our models can deal with input of any length in testing time. However, for the convenience of training the models with mini-batches, in the training stage we divide the waveforms in our training set into 10-second chunks (without overlaps) and use these chunks as the model input, leading to a matrix  of fixed size for each input.
In our implementation, we compute CQT with the librosa library , with 16,000 Hz sampling rate and 512-sample window size, again with no overlaps. We use a frequency scale of 88 bins, with 12 bins per octave to represent each note. Hence,  (bins) and  (frames).





[b]0.17
Pianoroll
[b]0.14
Instrument roll
[b]0.14
Pitch roll
Different symbolic representations of music.



Output
To provide temporal supervision, we use the pianorolls
as the target output of our models.(Strictly speaking, such a model is no longer an autoencoder, since the input and target output are different. We abuse the terminology for the short names of our models, e.g., Duo'AE.')
As depicted in Figure , a pianoroll is a binary-valued tensor that records the presence of notes (88 notes here) across time for each track (i.e., instrument) . When we consider  instruments, the target model output would be . 
 and  are temporally aligned, since we use MIDIs that are time-aligned with the audio clips to derive the pianorolls, as will be discussed in Section .

As shown in Figure , besides asking our models to generate  from the latent code of , we use the instrument roll  and pitch roll  as supervisory signals to disentangle timbre and pitch. As depicted in Figure , these two rolls can be obtained respectively by marginalizing a certain dimension of the pianoroll. 









The DuoAE Model

The architecture of DuoAE is illustrated in Figure (a). The designed is based on , but we use temporal supervision here and adapt the model to encode music.

Specifically, we train two encoders  and  to respectively convert  into the timbre code  and pitch code .
We note that, unlike in the case of image representation learning, here the latent codes are matrices, and we require that the second dimensions (i.e., ) represent time. This way, each column of  and  is a -dimensional representation of a temporal segment of the input. For abstraction, we require .

DuoAE also contains three decoders ,  and .  The encoders and decoders are trained such that we can use  to predict ,   to predict , and   to predict . The prediction error is measured by the cross entropy between the ground truth and the predicted one. For example, for the timbre classifier , it is:

where , 

'' denotes the element-wise product, and 
 is the sigmoid function that scales its input to . We can similarly define  and .


In each training epoch, we optimize both the encoders and decoders by minimizing ,  and  for the given training batch.
We refer to the way we train the model as using the temporal supervision, since to minimize the lost terms ,  and , we have to make accurate prediction for each of the  time frames.

























When the adversarial training strategy is employed (i.e., those marked by dashed lines in Figure (a)), we additionally consider the following two lost terms:


where , , meaning that we feed the 'wrong' input (purposefully) to  and .
Moreover,  and  are two matrices of all zeros. 
That is to say, when we use the wrong input, we expect  and  can output nothing (i.e., all zeros), since, e.g.,  is supposed not to contain any timbre-related information.








Please note that, in adversarial training, we use  and  to update the encoders only. This is to preserve the function of the decoders in making accurate predictions.


The UnetAE Model
The architecture of UnetAE is depicted in Figure (b).(We explain the name 'Unet' below. When the design of the encoder (E) and decoder (D) is symmetric, meaning that they have the same number of layers and that they use the same kernel sizes and stride sizes in the corresponding layers, we can add skip connections between the corresponding layers of E and D, by concatenating the output of the -th layer of E to the input of the -th last layer of D in the channel-wise direction. In this way, lower-layer information of E (closer to the input) can be directly passed to the higher-layer of D (closer to the output), making it easier to train deeper AEs. Because the resulting architecture has a U-shape, people refer to it as a U-net .)
In UnetAE, we learn only one encoder  to get a single latent representation  of the input . 
We add skip connections between  and  and 
learn  and  by minimizing , the cross entropy between  and the pianoroll . Moreover, we promote timbre information in  by refining  and learning a classifier  by minimizing  (see Eq. ()). When the adversarial training strategy is adopted, we use the classifier  pre-trained from DuoAE to further dispel pitch information from , by updating  (but fixing ) to minimize  (see Eq. ()).

In summary, we use , , and optionally , to train the encoder ; use  to train the decoder ; and use  to train the timbre classifier . 
We discard the lost term  when we do not use adversarial training.

The two key design principals of UnetAE are as follows. First, since  is supposed not to have any pitch information, the only way to obtain the pitch information needed to predict  is from the skip connections. 
Second, there are reasons to believe that the skip connections can pass along the pitch information, because there is nice one-to-one time-frequency correspondence between  and each frontal slice of ,(That is, both  and  refer to the activity of the same musical note  for the same time frame .) and because in  pitch only affects the lowest partial of a harmonic series created by a musical note, while timbre affects all the partials. If we view pitch as the boundary outlining an object (i.e., the harmonic series) and timbre as the texture of that object, it makes sense to use a U-net structure, since U-net performs well in image segmentation .






















*[t]
    The architecture of the proposed model for music audio editing, using UnetAE for disentangling timbre and pitch.
    


Discussion
The major difference between DuoAE and UnetAE is that there is no pitch code  in UnetAE. We argue below why this may be fine for music editing and generation applications.

As discussed in the introduction, pitch determines what (i.e., the notes) to be played in a music piece, whereas timbre determines how they would sound like to a listener. While the pitch content of a music piece can be largely specified on the musical score (i.e., symbolic notations of music), the timbre content manifests itself in the audio sounds. Therefore, we can learn the timbre code  from an input audio representation such as the waveform, STFT, or CQT. If we want to learn the pitch code , we can learn it from symbolic representations of music, as pursued in existing work on symbolic-domain music generation . 
Since the pitch code can be learned from musical scores, we may focus on learning the timbre code from music audio.

Moreover, for music audio editing, we are interested in manipulating the instrumentation of a clip without much affecting its pitch content. For this purpose, it may be sufficient to have the timbre code, since in UnetAE the pitch content can flow directly from the encoder to the decoder. On the other hand, to manipulate the pitch content without affecting the timbre, we can change the pitch in the symbolic domain and then use the same timbre code for decoding.


Implementation Details

Since the input and target output are both matrices (or tensors), we use convolutional layers in all the encoders and decoders of DuoAE and UnetAE. To accommodate input of variable length, we adopt a fully-convolutional design , meaning that we do not use pooling layers at all. In the encoders, we achieve dimension reduction (i.e., reducing  to  and reducing  to ) by setting the stride sizes of the kernels larger than one. We use  kernels in every layers of the encoders. In the decoders, we use transposed convolution for upsampling. The kernel size is also . 
Moreover, we use leaky ReLU as the activation function and add batch normalization to all but the last layer of the decoders, where we use the sigmoid function. 
Both DuoAE and UnetAE are trained using stochastic gradient descend (SGD) with momentum 0.9. The initial learning rate is set to 0.01.



Proposed Model for Music Editing

The model we propose for music editing is shown in Figure . We use an additional encoder-decoder subnet  and  after UnetAE to convert pianorolls to the audio domain.(Such a pianoroll-to-audio conversion can be made with commercial synthesizers, but the resulting sounds tend to be deadpan.) Here we use the STFT spectrograms of the original input audio as the target output. 
The model is trained in an end-to-end fashion by additionally minimizing , the MSE between the groundtruth STFT  and the predicted one . 


Lastly, we use the Griffin-Lim algorithm to estimate the phase of the spectorgram and generate the audio.

Once the model is trained, timbre editing can be done by manipulating the timbre code  and then generate . And, pitch editing can be done by manipulating the intermediate pianoroll  and then generate .


There are three critical designs to make it work.
First, we concatenate  with , the output of . This is important since the input to  is pianorolls, but the pianorolls do not contain sufficient timbre information to generate realistic audio.
Second, we only use one skip connection between  and , because if we use too many skip connections the gradients from the STFT will affect the training of  and lead to noisy pianorolls .

Third, to further prevent the gradients from the STFT to affect the pianorolls, 

we use deterministic binary neurons (BN) 
to binarize the output of 
with a hard thresholding function.









Discussion

A few other models have been proposed to generate musical audio signals.

Many of them do not take auxiliary conditional signal to condition the generation process. These include auto-regressive models such as the WaveNet model 
and GAN-based models such as the WaveGAN model.








Some recent works started to explore the so-called score-to-audio music generation, where the audio generation model is given a musical score and is asked to render the score into sounds. 
While the model proposed by Hawthorne et al. deals with only piano music, both our music editing model and the PerformanceNet model proposed by Wang and Yang aim to generate music of more other instruments. 

However, our model is different from these two prior arts in that we are not dealing with score-to-audio music generation but actually audio editing, which converts an audio into another audio. 
We use the estimated pianoroll  in the midway of the generation process for controllability. 
As Figure  shows, our model takes not only the pianoroll  but also the timbre code  of the original audio as inputs. We find such a problem setting interesting and would try to further improve it in our future work, using for example WaveNet-based decoder or GAN training for better audio quality. 





























*

The performance of different models for transcription (i.e., pianoroll prediction) and timbre crossover, evaluated in terms of pitch accuracy (Acc) and the timbre histogram intersection (HI) rate. 
The last two columns are 'piano to violin+cello' conversion. We use 'w/o adv' to denote the cases without adversarial training, and 'Params' the total number of parameters. The 'baseline model' (see Section ) shown in the first row does not use timbre and pitch classifiers and adversarial training. 



































































Dataset
 
We build a new dataset with paired audio and MIDI files to train and evaluate the proposed models. 
This is done by crawling the MuseScore web forum (https://musescore.com/), obtaining around 350,000 unique MIDI files and the corresponding MP3 files. Most MP3 files were synthesized from the MIDIs with the MuseScore synthesizer by the uploaders.(Though synthesized audio may sounds different than realistic audio, the problem can be solved by domain adaptation. We take it as a future task and will not further discussed in this paper. Besides, we also tested the model on realistic music and found that the model performs well for many of them.) 
Hence, the audio and MIDIs are already time-aligned.
We further ensure temporal alignment by using the method proposed by.

We then convert the time-aligned MIDIs to pianorolls with the Pypianoroll package . 



We consider the following nine instruments in this work (i.e., ): piano, acoustic guitar, electrical guitar, trumpet, saxphone, bass, violin, cello and flute. They are chosen based on their popularity in modern music and MuseScore.(We exclude drums for they are non-pitched. And, we exclude the singing voices, for they are not transcribed in MIDIs.) 
The average length of the music clips is 2 minutes. 
We randomly pick 585-1000 clips per instrument for the training set, and 27-50 clips per instrument for the test set. 
As a result, the training and test sets are class-balanced.








Experiment

In what follows, we first evaluate the accuracy of our models in predicting the pianorolls. We then evaluate the result of timbre and pitch disentanglement by examining the learned embeddings and by the timbre crossover evaluation method.

Evaluation on Pianoroll Prediction

We first evaluate how well we can transcribe the pianorolls from audio.  Since , the
output of , is a real-valued tensor in ,

we further binarize it with a simple threshold picking algorithm  so that we can compare it with the groundtruth pianoroll , which is binary. We select the threshold (from 0.1, 0.15, , 0.95, in total 20 candidates) by maximizing the accuracy on a held-out validation set.
The accuracy (denoted as 'Acc') is calculated by comparing  and  per instrument (by calculating the proportion of true positives among the  entires) and then taking the average across the instruments. 






This way, we can measure the accuracy for both instrument and pitch prediction, since falsely predicting the instrument of a note would also reduce the number of true positives for instruments.


In addition to DuoAE and UnetAE, we consider a baseline model that uses only an encoder  and a decoder  to get the latent code . That is, not using additional timbre and pitch classifiers and adversarial training. 

The first three columns of Table  show the result. 
Both DuoAE and UnetAE perform much better than the baseline. 
In addition, UnetAE outperforms DuoAE, despite that UnetAE uses much fewer parameters than DuoAE.
We attribute this to the skip connections, which bring detailed information from the input to the decoding process and thereby help pitch localization. The best accuracy 0.431 is achieved by UnetAE, with adversarial training.
















































[h!]
[c]
Baseline model





[c]
DuoAE
[c]
UnetAE
t-SNE visualization  of the timbre code learned by different models (best viewed in color). Left: the timbre codes of instrument solos. Right: the timbre code of audio chunks with two instruments, or of manipulated solo chunks that are six semitones lower than the original ('6L'). The instruments are piano, electric guitar, saxphone, bass, violin, cello, flute, acoustic guitar, trumpet.











Evaluation on Disentanglement
t-SNE visualization of the learned timbre code 
The first thing we do to evaluate the performance of timbre and pitch disentanglement is via examining the timbre code , by 

projecting them from the -dimensional space to a 2-D space with 
distributed stochastic neighbor embedding (t-SNE) . 
We implement the t-SNE algorithm via Sklearn library with learning rate 20, perplexity 30 and iteration 1,000. 
The first column of Figure  shows the learned timbre code for audio chunks of instrument solos randomly picked from MuseScore.

We can see clear clusters of points from the result of DuoAE and UnetAE, which is favorable since an instrument solo involves only one instrument.


The second column of Figure  shows the learned timbre code for two more cases: 1) audio chunks with two instruments, picked from MuseScore; 2) audio chunks of instrument solos, but with pitch purposefully shifted lower by us. 
For the first case (in cyan), both DuoAE and UnetAE can nicely position the chunks in the middle of the two clusters of involved instruments.
For the second case (in pink), both DuoAE and UnetAE fail to position the chunks within the clusters of the involved instruments, suggesting that the learned timbre code is not perfectly pitch invariant. But, from the distance between the chunks and the corresponding clusters, it seems UnetAE performs slightly better.





  

*[!htp]




















[c]0.16
piaflu
[c]0.16
pia
[c]0.16
piaflubas
[c]0.16
vioa-g
   
[c]0.16
pia
[c]0.16
vioa-g


[c]0.16
viocelflu
[c]0.16
pia
[c]0.16
viocel
[c]0.16
flua-gbas
   
[c]0.16
flua-gbas
[c]0.16
tru
Demonstration of timbre crossover (best viewed in color). The source clips are (a), (e), (g) and (k), and the generated ones (i.e., after crossover by UnetAE) are those to the right of them. We see from (c) that UnetAE finds the low-pitched notes for the bass to play; from (d) that it knows to use the violin to play the melody (originally played by the flute) and the acoustic guitar to play the chords (originally played by the piano); and from (l) that it does not always work well-it picks new instruments to play when the 
timbre palettes of the source and target clips do not match. 


[Purple: flute (flu), Red: piano (pia), Black: bass (bas), Green: acoustic guitar (a-g), Light purple: trumpet (tru), Yellow: cello (cel), Brown: violin (vio)]. 



Timbre Crossover 
Secondly, we propose and employ a new evaluation method called timbre crossover to evaluate the result of disentanglement. Specifically, we exchange the timbre codes of two existing audio clips and then decode them, to see whether we can exchange their instrumentation without affecting the pitch content in the symbolic domain. 
For example, if clip A (the source clip) plays the flute and clip B (the target clip) plays the trumpet, we hope that after timbre crossover the new clip A' would use the trumpet to play the original tune.


For objective evaluation, we compare the pitch between the pianorolls of the source clip and the new clip to get the pitch Acc. 
Moreover, we present the activity of different instruments in a clip as an -bin histogram and compute the histogram intersection (HI) between the histograms computed from the pianorolls of the target clip and the new clip.







Both pitch accuracy and timbre HI are the higher the better.








Table  tabulates the result of the following crossover scenarios: 
'anythingpiano,' 'anythingguitar,' and 

'pianoviolincello.'
We compute the average result for 20 cases for each scenario.

We can see that UnetAE has better pitch accuracy while poorer HI. 
We attribute this to the fact that UnetAE does not have control over the skip connections-some timbre information may still flow through the skip connections. But, the advantage of UnetAE is its timbre code is more pitch-invariant. 
When doing crossover, the pitch content would subject to less changes. 
In contrast, DuoAE achieves higher HI, suggesting that its pitch code is more timbre-invariant. 

But, as its timbre code is not pitch-invariant, when doing crossover, some notes might disappear, due to timbre replacement, 
causing the low pitch accuracy.  


Figure  demonstrates the result of UnetAE. 
In general, UnetAE works well in changing the timbre without much affecting the pitch.
Please read the caption for details. 


Besides, we also adapt and evaluate the models proposed by and for timbre crossover. For the model proposed by, we consider 'S' as instrument and 'Z' as pitch and use pianorolls as the target output. For, we replace 'style' with pitch and 'class' with instrument. The output is also pianorolls instead of CQT. As Table  shows, they can only achieve 0.558 and 0.295 Pitch Acc for 'anythingpiano,' and 0.537 and 0.279 Pitch Acc for 'anythingguitar.' This poor result is expected, since these models were not designed for music disentanglement.







The spectrogram of a note from a piano clip (left) and the one after timbre crossover (right) by the UnetAE-based model shown in Figure . The target timbre is violin.




Audio-domain Music Editing 


Finally, we demonstrate the result of timbre crossover in the audio domain, using the model depicted in Figure . 

Figure  shows the spectrograms of a note chunked from the original and generated audio clips, for pianoviolin crossover.
It is known that, compared to piano, violin has longer sustain after the attack, and stronger energy at the harmonics. We can see such characteristics in the generated spectrogram. However, so far we note that the model may not be sophisticated enough (e.g., perhaps we can additionally use a GAN-based loss) so the audio quality has room for improvement, but it shows that music audio editing is feasible.













Ablation Study

The two models 'w/o adv' (i.e., without adversarial training) in Table  can be viewed as ablated versions of the proposed models. We report some more ablation study below.

We replace the pianorolls with CQT as the target output for DuoAE. In our study, we found that predicting pianorolls consistently outperforms predicting CQT in both instrument recognition accuracy (by 9.6) and pitch accuracy (by 5.3). Using the pianorolls as the target performs much better, for it provides the benefits of "temporal supervision" claimed as the first contribution of the paper in our introduction section. We therefore decided to use the pianorolls as the target output for both DuoAE and UnetAE. 

Moreover, we also remove skip connection from the UnetAE. By doing so, the transcription accuracy drops to 0.407. Moreover, since there is no skip connection to help disentangle pitch and timbre, the pitch Acc of timbre crossover would drop from 0.691 to 0.068.

Conclusion and Discussion

In this paper, we have presented two encoder/decoder models to learn disentangled representation of musical timbre and pitch.
The core contribution is that the input to the models is an audio clip, and as a result it can change the timbre or pitch content of the audio clip by manipulating the learned timbre and pitch representations.
We can evaluate the result of disentanglement by generating new pianorolls or audio clips. 


We also proposed a new evaluation method called "timbre crossover" to analyze timbre and pitch disentanglement. Through timbre exchanging, analysis shows that UnetAE has better ability to create new instrumentation without losing pitch information. We also extend UnetAE to audio-domain editing. Result shows that it is feasible to change audio timbre through a deep network structure. 



Although our study shows that UnetAE can perform better than DuoAE in terms of transcription accuracy and timbre crossover, 
a weakness of UnetAE is that it has limited control over the skip connections and as a result part of the timbre information may also flow through skip connection. Besides, UnetAE does not learn a pitch representation, so the model has little control of the pitch information. 

The loss function for adversarial training is another topic for future study. Currently, we predict the zero-metrics  and  to dispel information from the embeddings. Other loss function, such as the one proposed in, can also be tested. Besides, it is important to have further evaluation on the timbre embedding. For example, from the timbre crossover we learn that timbre embedding somehow learns the relation between pitch and timbre,

but Figure  shows that it is not feasible to play the bass and chord with the trumpet. Further experiment can be done to test on variety of music genre and instrument combination. 


The proposed timbre crossover method holds the promise to help human or AI composer decide the instruments to play different parts of a given lead sheet or pianoroll. 
However, a drawback of the current model is that the timbre embeddings have to be picked from another music piece, which is less convenient in real-life usage. A more convenient scenario is that we can interpolate the existing embeddings and directly decide the instruments to use. A possible way to improve the model regarding this is to add one-hot vector for conditional learning. The one-hot vector can then be used for controlling the instrument usage.

The proposed audio editing model, though likely being the first one of its kind, is not sophisticated enough to synthesize realistic audio. The model can be improved by using  WaveNet as the decoder, or by using the multi-band structure proposed by Wang and Yang.





IEEE

10

Asif Ghias, Jonathan Logan, David Chamberlin, and Brian C. Smith,
"Query by humming: Musical information retrieval in an audio
  database,"
in Proc. ACM Multimedia, 1995, pp. 231-236.

Joan Serra, Emilia Gomez, Perfecto Herrera, and Xavier Serra,
"Chroma binary similarity and local alignment applied to cover song
  identification,"
IEEE Trans. Audio, Speech, and Language Processing, vol. 16,
  no. 6, pp. 1138-1151, 2008.

Meinard Muller, Daniel P. W. Ellis, Anssi Klapuri, and Gael Richard,
"Signal processing for music analysis,"
IEEE J. Selected Topics in Signal Processing, vol. 5, no. 6,
  pp. 1088-1110, 2011.

Meinard Muller and Sebastian Ewert,
"Towards timbre-invariant audio features for harmony-based music,"
IEEE Trans. Audio, Speech, and Language Processing, vol. 18,
  no. 3, pp. 649-662, 2010.

Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan,
  and Dumitru Erhan,
"Domain separation networks,"
in Proc. Advances in Neural Information Processing Systems,
  2016, pp. 343-351.

Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, and Xiaogang Wang,
"Exploring disentangled feature representation beyond face
  identification,"
in Proc. IEEE Conf. Computer Vision and Pattern Recognition,
  2018, pp. 2080-2089.

Jose David Fernandez and Francisco Vico,
"AI methods in algorithmic composition: A comprehensive survey,"
J. Artificial Intelligence Research, vol. 48, no. 1, pp.
  513-582, 2013.

Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang,
"MuseGAN: Symbolic-domain music generation and accompaniment with
  multi-track sequential generative adversarial networks,"
in Proc. AAAI Conf. Artificial Intelligence, 2018.

Hao-Min Liu, Meng-Hsuan Wu, and Yi-Hsuan Yang,
"Lead sheet generation and arrangement via a hybrid generative
  model,"
in Proc. Int. Soc. Music Information Retrieval Conf., 2018,
Late-breaking paper.

Luan Tran, Xi Yin, and Xiaoming Liu,
"Disentangled representation learning GAN for pose-invariant face
  recognition,"
in Proc. IEEE Conf. Computer Vision and Pattern Recognition,
  2017, pp. 1283-1292.

Hao-Wen Dong, Wen-Yi Hsiao, and Yi-Hsuan Yang,
"Pypianoroll: Open source Python package for handling multitrack
  pianoroll,"
in Proc. Int. Soc. Music Information Retrieval Conf., 2018,
Late-breaking paper; [Online]
  https://github.com/salu133445/pypianoroll.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
"U-net: Convolutional networks for biomedical image segmentation,"
in Proc. Int. Conf. Medical Image Computing and
  Computer-Assisted Intervention. Springer, 2015, pp. 234-241.

Michael Mathieu, Junbo Zhao, Pablo Sprechmann, Aditya Ramesh, and Yann LeCun,
"Disentangling factors of variation in deep representations using
  adversarial training,"
in Proc. Advances in Neural Information Processing Systems,
  2016.

Hao-Wen Dong and Yi-Hsuan Yang,
"Convolutional generative adversarial networks with binary neurons
  for polyphonic music generation,"
in Proc. Int. Soc. Music Information Retrieval Conf., 2018.

Jonathan Masci, Ueli Meier, Dan Ciresan, and Jurgen Schmidhuber,
"Stacked convolutional auto-encoders for hierarchical feature
  extraction,"
in Proc. Int. Conf. Artificial Neural Networks, 2011, pp.
  52-59.

Rui Huang, Shu Zhang, Tianyu Li, and Ran He,
"Beyond face rotation: Global and local perception GAN for
  photorealistic and identity preserving frontal view synthesis,"
arXiv preprint arXiv:1704.04086, 2017.

Yang Liu, Zhaowen Wang, Hailin Jin, and Ian Wassell,
"Multi-task adversarial network for disentangled feature learning,"
in Proc. IEEE Conf. Computer Vision and Pattern Recognition,
  2018, pp. 3743-3751.

Brian Cheung, Jesse A Livezey, Arjun K Bansal, and Bruno A Olshausen,
"Discovering hidden factors of variation in deep networks,"
arXiv preprint arXiv:1412.6583, 2014.

Wei-Ning Hsu, Yu Zhang, and James Glass,
"Learning latent representations for speech generation and
  transformation,"
arXiv preprint arXiv:1704.04222, 2017.

Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Fei-Fei Li, and Juan Carlos Niebles,
"Learning to decompose and disentangle representations for video
  prediction,"
arXiv preprint arXiv:1806.04166, 2018.

Feng Liu, Ronghang Zhu, Dan Zeng, Qijun Zhao, and Xiaoming Liu,
"Disentangling features in 3D face shapes for joint face
  reconstruction and recognition,"
in Proc. IEEE Conf. Computer Vision and Pattern Recognition,
  2018, pp. 5216-5225.

Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck,
"A hierarchical latent vector model for learning long-term structure
  in music,"
in Proc. Int. Conf. Machine Learning, 2018, pp. 4361-4370.

Gino Brunner, Andres Konrad, Yuyi Wang, and Roger Wattenhofer,
"MIDI-VAE: Modeling dynamics and instrumentation of music with
  applications to style transfer,"
in Proc. Int. Soc. Music Information Retrieval Conf., 2018, pp.
  23-27.

Rachel M. Bittner, Brian McFee, Justin Salamon, Peter Li, and Juan P. Bello.,
"Deep salience representations for  tracking in polyphonic
  music,"
in Proc. Int. Soc. Music Information Retrieval Conf., 2017, pp.
  63-70.

Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic,
"Is object localization for free?-weakly-supervised learning with
  convolutional neural networks,"
in Proc. Int. Computer Vision and Pattern Recognition, 2015,
  pp. 685-694.

Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric
  Battenberg, and Oriol Nieto,
"librosa: Audio and music signal analysis in python.,"
in Proc. Python in Science Conf., 2015, pp. 18-25.

Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla,
"SegNet: A deep convolutional encoder-decoder architecture for
  image segmentation,"
IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 39,
  no. 12, pp. 2481-2495, 2017.

Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang,
"MidiNet: A convolutional generative adversarial network for
  symbolic-domain music generation,"
in Proc. Int. Soc. Music Information Retrieval Conf., 2017.

Ian Simon, Adam Roberts, Colin Raffel, Jesse Engel, Curtis Hawthorne, and
  Douglas Eck,
"Learning a latent space of multitrack measures,"
in Proc. Int. Soc. Music Information Retrieval Conf., 2018.

D. Griffin and Jae Lim,
"Signal estimation from modified short-time Fourier transform,"
IEEE Trans. Acoustics, Speech, and Signal Processing, vol. 32,
  no. 2, pp. 236-243, 1984.

Aaron V. D. Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
  Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W Senior, and Koray
  Kavukcuoglu,
"WaveNet: A generative model for raw audio,"
arXiv preprint arXiv:1609.03499, 2016.

Chris Donahue, Julian McAuley, and Miller Puckette,
"Synthesizing audio with generative adversarial networks,"
arXiv preprint arXiv:1802.04208, 2018.

Bryan Wang and Yi-Hsuan Yang,
"PerformanceNet: Score-to-audio music generation with multi-band
  convolutional residual network,"
in Proc. AAAI Conf. Artificial Intelligence, 2019.

Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, Cheng-Zhi Anna
  Huang, Sander Dieleman, Erich Elsen, Jesse Engel, and Douglas Eck,
"Enabling factorized piano music modeling and generation with the
  MAESTRO dataset,"
arXiv preprint arXiv:1810.12247, 2018.

Naama Hadad, Lior Wolf, and Moni Shahar,
"A two-step disentanglement method,"
in Proc. IEEE Conf. Computer Vision and Pattern Recognition,
  2018, pp. 772-780.

Colin Raffel and Daniel P. W. Ellis,
"Optimizing DTW-based audio-to-MIDI alignment and matching,"
in Proc. IEEE Int. Conf. Acoustics, Speech and Signal
  Processing, 2016, pp. 81-85.

Laurens van der Maaten and Geoffrey Hinton,
"Visualizing data using t-SNE,"
J. Machine Learning Research, vol. 9, no. Nov, pp. 2579-2605,
  2008.

Michael J. Swain and Dana H. Ballard,
"Color indexing,"
Int. J. Computer Vision, vol. 7, no. 1, pp. 11-32, 1991.

Kenichi Miyamoto, Hirokazu Kameoka, Takuya Nishimoto, Nobutaka Ono, and Shigeki
  Sagayama,
"Harmonic-temporal-timbral clustering (HTTC) for the analysis of
  multi-instrument polyphonic music signals,"
in Proc. IEEE Int. Conf. Acoustics, Speech and Signal
  Processing, 2008, pp. 113-116.


