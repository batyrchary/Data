Experiments

MNIST digit painting.
We constructed a dataset consisting of simple, templated instructions such as "Draw zero.", "Paint 5." etc. paired with random images for the corresponding MNIST digit.
Each instruction follows the template:   with  and  is specified either numerically ('0') or in word format ('zero'). This dataset has 60K image-instruction pairs (one instruction per image) in total. Following ganin2018synthesizing, we use the 'libmypaint' painting library as our rendering environment. The agent produces a sequence of strokes on a canvas  using a brush. Each action involves predicting 5 discrete quantities - end point and control point of the brush (on 32 x 32 grid), pressure applied to the brush (10 levels), brush size (4 levels) and a binary flag to choose between producing a stroke and skipping (not painting anything). So, the size of the action space is 83,886,080.

Figure fig:teaserb shows samples of MNIST digits painted by our agent when conditioned on the corresponding instructions. We can see that the agent draws the correct digit most of the time and for a given instruction, shows diversity in the generated samples, unlike the fixed pixel-based reward function baseline (L2) whose outputs do not show sufficient diversity (Figure fig:teaserd). Moreover, the baseline takes twice as much time as our agent before it starts generating MNIST-like images.

Quantitatively, to evaluate the correctness (whether the generated samples are consistent with the instruction) and diversity (for a given instruction, whether the generated samples are diverse), we report Inception ScoreDBLP:journals/corr/SalimansGZCRC16 (a combined score of quality and diversity) and Frechet Inception Distance (FID)DBLP:journals/corr/HeuselRUNKH17 (a measures of how close the generated image distribution is to the real image distribution) for 1000 samples using a pretrained MNIST classifier (Table tab:mnist_clevr_quant). We can see that our learned Discriminator outperforms the fixed pixel-based reward baseline (L2).

table[h]
center
0.90
tabular cccccc  
& 2cMNIST Digit Painting & 3c3D Scene Construction 

Approach & Inception Score & FID & Correctness & Diversity in Colors & Diversity in Sizes 

L2 & 1.22 & 283.5 & 1.4 & 0.77 & 0.09 
 
Discriminator & 1.39 & 259.7 & 6.8 & 1.48 & 0.53 
 
tabular

center

Quantitative evaluation of Discriminator and L2 (in pixel space). The former outperforms the latter on the two domains that we use in our experiments. Depending on the domain, we report Inception Score, Correctness, Diversities - the higher the better, and FID - the lower the better.

tab:mnist_clevr_quant
table

3D scene construction.
Inspired by  CLEVR johnson2017clevr, we constructed a dataset consisting of images of 3D scenes and textual instructions. Each scene consists of a 3D object (cylinder, sphere, cube) of varying sizes, colors that are placed at different locations on the 32 x 32 grid. 
The instructions are generated using templates
of the form "There is a ", where attributes are either colors (8 values) or sizes (2 values), and shapes are: cubes, spheres or cylinders. This dataset has 32,318 image-instruction pairs in total. We use a 3D editor as our rendering environment. The agent is provided with an empty scene and it adds objects sequentially. Each action involves predicting 5 discrete quantities - location of the object (on 32 x 32 grid), object shape, size and color and a flag to choose between adding a new object, changing the previously added object, or doing nothing. So, the size of the action space is 147,456.

Figure fig:teaser(a) shows samples of scenes generated by our agent when conditioned on the corresponding instructions. We can see that the generated scenes are consistent with the instruction and also show diversity in the attribute of the object which is not mentioned in the instruction (e.g., diversity in colors when the instruction is 'There is a small sphere.'). The fixed pixel-based reward function baseline (L2) (Figure fig:teaser(c)), fails poorly in following the instruction. 

Quantitatively, we evaluate the correctness and diversity for the unspecified attribute (for samples which are correct) as reported in Table tab:mnist_clevr_quant for 10 samples for each instruction. Each sample is judged as correct / incorrect based on human evaluation. For measuring the diversity, we compute the entropy of the corresponding attribute distribution (number of correct samples for each of the 8 colors / 2 sizes). We can see that our learned Discriminator outperforms the L2 baseline.

Introduction

Virtual worlds are being increasingly used to make advancements in deep reinforcement learning researchganin2018synthesizing, bahdanau2018learning, embodiedqa, DBLP:journals/corr/HermannHGWFSSCJ17. However, these worlds are typically hand-designed, 
and therefore their construction at scale is expensive.
In this work, we build generative agents that output programs for diverse scenes that can be used to create virtual worlds, conditioned on an  instruction. Thus, with a short text string, we can obtain a distribution over multiple diverse virtual scenes, each consistent with the  instruction. We experiment with two instruction-conditioned domains: (1) drawing MNIST digits with a paint software, and (2) constructing scenes in a 3D editor; both are shown in Figure fig:teaser.

figure*[t]
center
0.8
figs/teaser_v2.pdf

center

We build agents that can generate programs for diverse scenes conditioned on a given symbolic instruction. (a) Conditioned on a given instruction (e.g., "There is a small sphere".), the agent learns to generate programs for 3D scenes such that all scenes have a small sphere. We can see that the generated scenes show diversity in unspecified attributes (color and location of the sphere). (b) Conditioned on a simple instruction (e.g., "Draw zero", "Paint 1", etc.), the agent learns to generate programs that draw the corresponding
MNIST digits. We can see that the generated drawings of a given label show diversity in style. (c) and (d): Results corresponding to (a) and (b) respectively for the fixed pixel-based reward function baseline. We can see that the scenes / drawings rendered from the generated programs are either not consistent with the input instruction (c) or do not show sufficient diversity (d).
fig:teaser
figure*


We build our agents on top of recently introduced reinforced adversarial learning frameworkpganin2018synthesizing, where final goals are specified to the agent via images of the virtual scenes. Unlikepganin2018synthesizing where, either random noise (unconditional generation) or the goal image (reconstruction) is used as the conditioning input, in our work, a symbolic instruction is used as the conditioning input for our policies. Since, a single instruction corresponds to a diverse set of, yet consistent goal images, the agent needs to learn to generate a controlled distribution over programs such that all programs are consistent with the instruction. A discriminator that takes an image (generated or an example goal image) and the corresponding instruction acts as a reward learning function to provide training signal to the generator. Thus, the generator learns to generate the distribution of programs consistent with the symbolic instruction. In particular, this setting leads to more accurate and more diverse programs than hand-crafted discriminators such as L2-distance (in pixel space) between the generated and goal images.

Language conditioned policies with adversarial reward induction has also been explored inpbahdanau2018learning. However,pbahdanau2018learning do not evaluate whether the agent is capable of capturing the diversity in the programs. Moreover, we train our agents on more visually complex tasks involving 3D scenes.

Our work also draws inspiration from Visual Question Answering and Analysis-by-Synthesis:

Visual Question Answering. Visual question answering is a well-established subfield of computer vision that tests a machine's understanding of vision, language, spatial arrangement of objects, knowledge based and commonsense reasoning via question-answer pairs about images malinowski2014multi,malinowski2014towards,geman2015visual,antol2015vqa,vqa_ijcv,johnson2017clevr,agrawal2017c,agrawal2018don. Our work draws inspiration from this line of research, especiallyjohnson2017clevr by focusing on similar aspects of the scene comprehension. 

However, in our work, the machine's task is not to generate an answer, but to generate a scene by sequentially generating programs. The latter involves very different challenges (e.g., reward learning, modeling diversity in generated outputs, operating in rich action space). 
(e.g., reward learning, modeling diversity in generated outputs, operating in a rich action space).
Finally, we also believe that architectures based on latent programs, or simulators are under-represented in the visual question answering community malinowski2014multi,chowdhury2016contextual,johnson2017inferring,DBLP:journals/corr/HuARDS17,mascharkatransparency,wagner2018answering even though they offer clear benefits such as interpretability, compositionality, and can potentially generalize better to new instances. This aspect of generalization has recently been recognized as one of the major bottlenecks of the current state of affairs agrawal2018don.

Analysis-by-Synthesis.
Representing images or videos by latent programs 
has also shown promise in learning meaningful representations of data yi2018neural,nair2008analysis,kulkarni2015picture,kulkarni2015deep,kulkarni2015picture,malinowski2014multi,wu2017learning,ganin2018synthesizing,bahdanau2018learning. 
Such frameworks not only improve the interpretability of otherwise obscured decision making systems, but also have the potential to generalize better to unseen data, owing to stronger inductive biases. For instance, pkulkarni2015picture use latent programs to infer novels poses of 3D objects,pwu2017learning show how to invert physical simulation which in turn can be used for inferring objects' dynamics, andpyi2018neural achieve the state-of-the-art on the CLEVR visual question answering problem.  

Contributions. We (1) extend the recently proposed reinforced adversarial learning frameworkganin2018synthesizing to instruction conditioned policies, (2) present the experimental results on two instruction-conditioned domains (drawing MNIST digits with a paint software and constructing scenes in a 3D editor), and (3) show that the proposed setting leads to more accurate and more diverse programs than a fixed pixel-based reward function baseline. 





Generating Diverse Programs with Instruction Conditioned Reinforced Adversarial Learning




  Aishwarya Agrawal 

  Georgia TechThis work was done while Aishwarya was interning at DeepMind.

  aishwarya@gatech.edu 

    Mateusz Malinowski 

  DeepMind

  mateuszm@google.com 

    Felix Hill 

  DeepMind

  felixhill@google.com 

    Ali Eslami 

  DeepMind

  aeslami@google.com 

    Oriol Vinyals 

  DeepMind

  vinyals@google.com 

    Tejas Kulkarni 

  DeepMind

  tkulkarni@google.com 








Advances in Deep Reinforcement Learning have led to agents that perform well across a variety of sensory-motor domains. In this work, we study the setting in which an agent must learn to generate programs for diverse scenes conditioned on a given symbolic instruction.
Final goals are specified to our agent via images of the scenes. A symbolic instruction consistent with the goal images is used as the conditioning input for our policies. 
Since a single instruction corresponds to a diverse set of different but still consistent end-goal images, the agent needs to learn to generate a distribution over programs given an instruction. 
We demonstrate that with simple changes to the reinforced adversarial learning objective, we can learn instruction conditioned policies to achieve the corresponding diverse set of goals. Most importantly, our agent's stochastic policy is shown to more accurately capture the diversity in the goal distribution than a fixed pixel-based reward function baseline. 
We demonstrate the efficacy of our approach on two domains: (1) drawing MNIST digits with a paint software conditioned on instructions and (2) constructing scenes in a 3D editor that satisfies a certain instruction. 








Acknowledgments: The authors wish to acknowledge Igor Babuschkin, Caglar Gulcehre, Pushmeet Kohli, Piotr Trochim, and Antonio Garcia for their advice in this project.

nips2018_conference

Approach

*[h]





An overview of our approach. 
Given an instruction
the generator (policy network) outputs a program which is rendered by a non-differentiable (denoted by dashed arrows) renderer into an image. This process repeats for a fixed number of time steps. The final image is fed to a discriminator, along with the instruction (the same that is given to the generator), that produces a reward.


Our goal is to train a generator agent  that is capable of generating programs for diverse scenes () conditioned on an instruction  that are indistinguishable from the ground truth data samples . Towards this goal, we frame our problem within the reinforcement adversarial learning setting goodfellow2014generative,mnih2016asynchronous. 
The generator is modeled by a recurrent policy network  which, at every time step, takes a symbolic instruction as input, and outputs a program which is executed by a renderer to produce an image (Figure  shows the whole pipeline). For instance, if the instruction is "Draw zero", the generator uses the available painting tools (location, pressure, and brush size) to sequentially paint the digit 0 on the canvas. Similarly, with the input "There is a red sphere" the generator manipulates the scene by adding / changing objects so that the resulting scene has a red sphere.
The role of the discriminator  is to decide if the distribution over generated images given an instruction () follows the ground truth distribution of images given the instruction ().

*[t]



(a) Discriminator's architecture. It takes as inputs the instruction and an image (either the image generated by the agent or a goal image from the dataset), learns a joint embedding, and outputs a scalar score evaluating the 'realness' of the input pair. (b) Policy Network's architecture, unrolled over time. At each time step, it takes - an instruction, the rendered image from the previous time step (blank canvas initially) and the previous action, learns a joint embedding and passes it to an LSTM whose output is fed to a decoder which samples the actions.


Discriminator.
To train the discriminator, we use LSTM hochreiter1997long to transform the text instruction into a fixed-length vector representation. 
Using CNNs, we transform the input image into a spatial tensor. 
Inspired by architectures used in visual question answering malinowski2018visualqadevil and text-to-image Generative Adversarial Networks DBLP:journals/corr/ReedAYLSL16, we concatenate each visual spatial vector with the replicated LSTM vector, and process both modalities by 4 layers of 1-by-1 CNNs. 
These representations are then summarized by a permutation-invariant sum-pooling operator followed by an MLP that produces a scalar score which is used as the reward for the generator agent (Figure a).
We train the discriminator by minimizing the minimax objective from gan_goodfellow:
, where  denotes gradient penalty, gently enforcing Lipschitz continuity gulrajani2017improved.

Generator. 
Generator's goal is to generate programs that lead to scenes that match data distribution . Here, programs are defined as a sequence of commands . Every command in this sequence is transformed into scenes through a domain-specific transformation . For instance, if  is a 3D renderer, and  is a program that describes a scene,  is the image created by executing the program by the renderer. The loss function for the generator is set up adversarially to that for , i.e. .
Due to the non-differentiability of the renderer, we train the network with the REINFORCE williams1992simple and advantage actor-critic (A2C), , following ganin2018synthesizing. Figure b depicts the architecture.